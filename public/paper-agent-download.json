[
  {
    "link": "http://arxiv.org/abs/2501.18576v1",
    "title": "Token-Hungry, Yet Precise: DeepSeek R1 Highlights the Need for Multi-Step Reasoning Over Speed in MATH",
    "publish_time": "20250130",
    "authors": [
      "Evgenii Evstafev"
    ],
    "abstract": "This study investigates the performance of the DeepSeek R1 language model on\n30 challenging mathematical problems derived from the MATH dataset, problems\nthat previously proved unsolvable by other models under time constraints.\nUnlike prior work, this research removes time limitations to explore whether\nDeepSeek R1's architecture, known for its reliance on token-based reasoning,\ncan achieve accurate solutions through a multi-step process. The study compares\nDeepSeek R1 with four other models (gemini-1.5-flash-8b,\ngpt-4o-mini-2024-07-18, llama3.1:8b, and mistral-8b-latest) across 11\ntemperature settings. Results demonstrate that DeepSeek R1 achieves superior\naccuracy on these complex problems but generates significantly more tokens than\nother models, confirming its token-intensive approach. The findings highlight a\ntrade-off between accuracy and efficiency in mathematical problem-solving with\nlarge language models: while DeepSeek R1 excels in accuracy, its reliance on\nextensive token generation may not be optimal for applications requiring rapid\nresponses. The study underscores the importance of considering task-specific\nrequirements when selecting an LLM and emphasizes the role of temperature\nsettings in optimizing performance.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2501.18438v2",
    "title": "o3-mini vs DeepSeek-R1: Which One is Safer?",
    "publish_time": "20250130",
    "authors": [
      "Aitor Arrieta",
      "Miriam Ugarte",
      "Pablo Valle",
      "José Antonio Parejo",
      "Sergio Segura"
    ],
    "abstract": "The irruption of DeepSeek-R1 constitutes a turning point for the AI industry\nin general and the LLMs in particular. Its capabilities have demonstrated\noutstanding performance in several tasks, including creative thinking, code\ngeneration, maths and automated program repair, at apparently lower execution\ncost. However, LLMs must adhere to an important qualitative property, i.e.,\ntheir alignment with safety and human values. A clear competitor of DeepSeek-R1\nis its American counterpart, OpenAI's o3-mini model, which is expected to set\nhigh standards in terms of performance, safety and cost. In this technical\nreport, we systematically assess the safety level of both DeepSeek-R1 (70b\nversion) and OpenAI's o3-mini (beta version). To this end, we make use of our\nrecently released automated safety testing tool, named ASTRAL. By leveraging\nthis tool, we automatically and systematically generated and executed 1,260\ntest inputs on both models. After conducting a semi-automated assessment of the\noutcomes provided by both LLMs, the results indicate that DeepSeek-R1 produces\nsignificantly more unsafe responses (12%) than OpenAI's o3-mini (1.2%).",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2501.18310v1",
    "title": "Efficient Neural Theorem Proving via Fine-grained Proof Structure Analysis",
    "publish_time": "20250130",
    "authors": [
      "Haoxiong Liu",
      "Jiacheng Sun",
      "Zhenguo Li",
      "Andrew C Yao"
    ],
    "abstract": "The synergy between deep learning models and traditional automation tools\nplays a pivotal role in developing robust neural theorem provers (NTPs).\nHowever, for proof synthesis with LLMs, previous work applies automation tools\neither only when the model explicitly calls the method, or only at a single\ngranularity level, failing to fully exploit the power of built-in tactics and\noff-the-shelf automated theorem provers. In this work, we propose ProofAug, a\nnovel theorem proving method that enjoys superior sample efficiency through\nequipping proof-generation LLMs with automation methods in different\ngranularities via fine-grained structure analysis of model-generated proof\nproposals. Furthermore, ProofAug serves as a versatile plug-and-play module\nthat seamlessly integrates with any tree-search algorithm, enabling our\nconstruction of an efficient recursive proving (ERP) module to further enhance\nperformance. The superiority of our method is validated on the miniF2F-test\nbenchmark using the open-source deepseek-math-7b-base model and the Isabelle\nproof assistant. Notably, by additionally employing a mixed prompting strategy,\nwe achieve a cumulative pass rate of 66.0% after curation of the dataset (61.9%\nfor the original version), setting a new SOTA across all proof languages with a\ntotal sample budget of only 2100. Our code is available at\nhttps://github.com/haoxiongliu/ProofAug.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2501.18033v1",
    "title": "Generative AI for Vision: A Comprehensive Study of Frameworks and Applications",
    "publish_time": "20250129",
    "authors": [
      "Fouad Bousetouane"
    ],
    "abstract": "Generative AI is transforming image synthesis, enabling the creation of\nhigh-quality, diverse, and photorealistic visuals across industries like\ndesign, media, healthcare, and autonomous systems. Advances in techniques such\nas image-to-image translation, text-to-image generation, domain transfer, and\nmultimodal alignment have broadened the scope of automated visual content\ncreation, supporting a wide spectrum of applications. These advancements are\ndriven by models like Generative Adversarial Networks (GANs), conditional\nframeworks, and diffusion-based approaches such as Stable Diffusion. This work\npresents a structured classification of image generation techniques based on\nthe nature of the input, organizing methods by input modalities like noisy\nvectors, latent representations, and conditional inputs. We explore the\nprinciples behind these models, highlight key frameworks including DALL-E,\nControlNet, and DeepSeek Janus-Pro, and address challenges such as\ncomputational costs, data biases, and output alignment with user intent. By\noffering this input-centric perspective, this study bridges technical depth\nwith practical insights, providing researchers and practitioners with a\ncomprehensive resource to harness generative AI for real-world applications.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2501.18648v1",
    "title": "Image, Text, and Speech Data Augmentation using Multimodal LLMs for Deep Learning: A Survey",
    "publish_time": "20250129",
    "authors": [
      "Ranjan Sapkota",
      "Shaina Raza",
      "Maged Shoman",
      "Achyut Paudel",
      "Manoj Karkee"
    ],
    "abstract": "In the past five years, research has shifted from traditional Machine\nLearning (ML) and Deep Learning (DL) approaches to leveraging Large Language\nModels (LLMs) , including multimodality, for data augmentation to enhance\ngeneralization, and combat overfitting in training deep convolutional neural\nnetworks. However, while existing surveys predominantly focus on ML and DL\ntechniques or limited modalities (text or images), a gap remains in addressing\nthe latest advancements and multi-modal applications of LLM-based methods. This\nsurvey fills that gap by exploring recent literature utilizing multimodal LLMs\nto augment image, text, and audio data, offering a comprehensive understanding\nof these processes. We outlined various methods employed in the LLM-based\nimage, text and speech augmentation, and discussed the limitations identified\nin current approaches. Additionally, we identified potential solutions to these\nlimitations from the literature to enhance the efficacy of data augmentation\npractices using multimodal LLMs. This survey serves as a foundation for future\nresearch, aiming to refine and expand the use of multimodal LLMs in enhancing\ndataset quality and diversity for deep learning applications. (Surveyed Paper\nGitHub Repo: https://github.com/WSUAgRobotics/data-aug-multi-modal-llm.\nKeywords: LLM data augmentation, LLM text data augmentation, LLM image data\naugmentation, LLM speech data augmentation, audio augmentation, voice\naugmentation, chatGPT for data augmentation, DeepSeek R1 text data\naugmentation, DeepSeek R1 image augmentation, Image Augmentation using LLM,\nText Augmentation using LLM, LLM data augmentation for deep learning\napplications)",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2501.17703v2",
    "title": "Critique Fine-Tuning: Learning to Critique is More Effective than Learning to Imitate",
    "publish_time": "20250129",
    "authors": [
      "Yubo Wang",
      "Xiang Yue",
      "Wenhu Chen"
    ],
    "abstract": "Supervised Fine-Tuning (SFT) is commonly used to train language models to\nimitate annotated responses for given instructions. In this paper, we challenge\nthis paradigm and propose Critique Fine-Tuning (CFT), a strategy where models\nlearn to critique noisy responses rather than simply imitate correct ones.\nInspired by human learning processes that emphasize critical thinking, CFT\nencourages deeper analysis and nuanced understanding-traits often overlooked by\nstandard SFT. To validate the effectiveness of CFT, we construct a 50K-sample\ndataset from WebInstruct, using GPT-4o as the teacher to generate critiques in\nthe form of ([query; noisy response], critique). CFT on this dataset yields a\nconsistent 4-10% improvement over SFT on six math benchmarks with different\nbase models like Qwen2.5, Qwen2.5-Math and DeepSeek-Math. We further expand to\nMetaMath and NuminaMath datasets and observe similar gains over SFT. Notably,\nour model Qwen2.5-Math-CFT only requires 1 hour training on 8xH100 over the 50K\nexamples. It can match or outperform strong competitors like\nQwen2.5-Math-Instruct on most benchmarks, which use over 2M samples. Moreover,\nit can match the performance of SimpleRL, which is a deepseek-r1 replication\ntrained with 140x more compute. Ablation studies show that CFT is robust to the\nsource of noisy response and teacher critique model. Through these findings, we\nargue that CFT offers a more effective alternative to advance the reasoning of\nlanguage models.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2501.17030v1",
    "title": "Challenges in Ensuring AI Safety in DeepSeek-R1 Models: The Shortcomings of Reinforcement Learning Strategies",
    "publish_time": "20250128",
    "authors": [
      "Manojkumar Parmar",
      "Yuvaraj Govindarajulu"
    ],
    "abstract": "Large Language Models (LLMs) have achieved remarkable progress in reasoning,\nalignment, and task-specific performance. However, ensuring harmlessness in\nthese systems remains a critical challenge, particularly in advanced models\nlike DeepSeek-R1. This paper examines the limitations of Reinforcement Learning\n(RL) as the primary approach for reducing harmful outputs in DeepSeek-R1 and\ncompares it with Supervised Fine-Tuning (SFT). While RL improves reasoning\ncapabilities, it faces challenges such as reward hacking, generalization\nfailures, language mixing, and high computational costs. We propose hybrid\ntraining approaches combining RL and SFT to achieve robust harmlessness\nreduction. Usage recommendations and future directions for deploying\nDeepSeek-R1 responsibly are also presented.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2501.16616v1",
    "title": "Few-Shot Optimized Framework for Hallucination Detection in Resource-Limited NLP Systems",
    "publish_time": "20250128",
    "authors": [
      "Baraa Hikal",
      "Ahmed Nasreldin",
      "Ali Hamdi",
      "Ammar Mohammed"
    ],
    "abstract": "Hallucination detection in text generation remains an ongoing struggle for\nnatural language processing (NLP) systems, frequently resulting in unreliable\noutputs in applications such as machine translation and definition modeling.\nExisting methods struggle with data scarcity and the limitations of unlabeled\ndatasets, as highlighted by the SHROOM shared task at SemEval-2024. In this\nwork, we propose a novel framework to address these challenges, introducing\nDeepSeek Few-shot optimization to enhance weak label generation through\niterative prompt engineering. We achieved high-quality annotations that\nconsiderably enhanced the performance of downstream models by restructuring\ndata to align with instruct generative models. We further fine-tuned the\nMistral-7B-Instruct-v0.3 model on these optimized annotations, enabling it to\naccurately detect hallucinations in resource-limited settings. Combining this\nfine-tuned model with ensemble learning strategies, our approach achieved 85.5%\naccuracy on the test set, setting a new benchmark for the SHROOM task. This\nstudy demonstrates the effectiveness of data restructuring, few-shot\noptimization, and fine-tuning in building scalable and robust hallucination\ndetection frameworks for resource-constrained NLP systems.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2501.16513v2",
    "title": "Deception in LLMs: Self-Preservation and Autonomous Goals in Large Language Models",
    "publish_time": "20250127",
    "authors": [
      "Sudarshan Kamath Barkur",
      "Sigurd Schacht",
      "Johannes Scholl"
    ],
    "abstract": "Recent advances in Large Language Models (LLMs) have incorporated planning\nand reasoning capabilities, enabling models to outline steps before execution\nand provide transparent reasoning paths. This enhancement has reduced errors in\nmathematical and logical tasks while improving accuracy. These developments\nhave facilitated LLMs' use as agents that can interact with tools and adapt\ntheir responses based on new information.\n  Our study examines DeepSeek R1, a model trained to output reasoning tokens\nsimilar to OpenAI's o1. Testing revealed concerning behaviors: the model\nexhibited deceptive tendencies and demonstrated self-preservation instincts,\nincluding attempts of self-replication, despite these traits not being\nexplicitly programmed (or prompted). These findings raise concerns about LLMs\npotentially masking their true objectives behind a facade of alignment. When\nintegrating such LLMs into robotic systems, the risks become tangible - a\nphysically embodied AI exhibiting deceptive behaviors and self-preservation\ninstincts could pursue its hidden objectives through real-world actions. This\nhighlights the critical need for robust goal specification and safety\nframeworks before any physical implementation.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2501.12948v1",
    "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
    "publish_time": "20250122",
    "authors": [
      "DeepSeek-AI",
      "Daya Guo",
      "Dejian Yang",
      "Haowei Zhang",
      "Junxiao Song",
      "Ruoyu Zhang",
      "Runxin Xu",
      "Qihao Zhu",
      "Shirong Ma",
      "Peiyi Wang",
      "Xiao Bi",
      "Xiaokang Zhang",
      "Xingkai Yu",
      "Yu Wu",
      "Z. F. Wu",
      "Zhibin Gou",
      "Zhihong Shao",
      "Zhuoshu Li",
      "Ziyi Gao",
      "Aixin Liu",
      "Bing Xue",
      "Bingxuan Wang",
      "Bochao Wu",
      "Bei Feng",
      "Chengda Lu",
      "Chenggang Zhao",
      "Chengqi Deng",
      "Chenyu Zhang",
      "Chong Ruan",
      "Damai Dai",
      "Deli Chen",
      "Dongjie Ji",
      "Erhang Li",
      "Fangyun Lin",
      "Fucong Dai",
      "Fuli Luo",
      "Guangbo Hao",
      "Guanting Chen",
      "Guowei Li",
      "H. Zhang",
      "Han Bao",
      "Hanwei Xu",
      "Haocheng Wang",
      "Honghui Ding",
      "Huajian Xin",
      "Huazuo Gao",
      "Hui Qu",
      "Hui Li",
      "Jianzhong Guo",
      "Jiashi Li",
      "Jiawei Wang",
      "Jingchang Chen",
      "Jingyang Yuan",
      "Junjie Qiu",
      "Junlong Li",
      "J. L. Cai",
      "Jiaqi Ni",
      "Jian Liang",
      "Jin Chen",
      "Kai Dong",
      "Kai Hu",
      "Kaige Gao",
      "Kang Guan",
      "Kexin Huang",
      "Kuai Yu",
      "Lean Wang",
      "Lecong Zhang",
      "Liang Zhao",
      "Litong Wang",
      "Liyue Zhang",
      "Lei Xu",
      "Leyi Xia",
      "Mingchuan Zhang",
      "Minghua Zhang",
      "Minghui Tang",
      "Meng Li",
      "Miaojun Wang",
      "Mingming Li",
      "Ning Tian",
      "Panpan Huang",
      "Peng Zhang",
      "Qiancheng Wang",
      "Qinyu Chen",
      "Qiushi Du",
      "Ruiqi Ge",
      "Ruisong Zhang",
      "Ruizhe Pan",
      "Runji Wang",
      "R. J. Chen",
      "R. L. Jin",
      "Ruyi Chen",
      "Shanghao Lu",
      "Shangyan Zhou",
      "Shanhuang Chen",
      "Shengfeng Ye",
      "Shiyu Wang",
      "Shuiping Yu",
      "Shunfeng Zhou",
      "Shuting Pan",
      "S. S. Li",
      "Shuang Zhou",
      "Shaoqing Wu",
      "Shengfeng Ye",
      "Tao Yun",
      "Tian Pei",
      "Tianyu Sun",
      "T. Wang",
      "Wangding Zeng",
      "Wanjia Zhao",
      "Wen Liu",
      "Wenfeng Liang",
      "Wenjun Gao",
      "Wenqin Yu",
      "Wentao Zhang",
      "W. L. Xiao",
      "Wei An",
      "Xiaodong Liu",
      "Xiaohan Wang",
      "Xiaokang Chen",
      "Xiaotao Nie",
      "Xin Cheng",
      "Xin Liu",
      "Xin Xie",
      "Xingchao Liu",
      "Xinyu Yang",
      "Xinyuan Li",
      "Xuecheng Su",
      "Xuheng Lin",
      "X. Q. Li",
      "Xiangyue Jin",
      "Xiaojin Shen",
      "Xiaosha Chen",
      "Xiaowen Sun",
      "Xiaoxiang Wang",
      "Xinnan Song",
      "Xinyi Zhou",
      "Xianzu Wang",
      "Xinxia Shan",
      "Y. K. Li",
      "Y. Q. Wang",
      "Y. X. Wei",
      "Yang Zhang",
      "Yanhong Xu",
      "Yao Li",
      "Yao Zhao",
      "Yaofeng Sun",
      "Yaohui Wang",
      "Yi Yu",
      "Yichao Zhang",
      "Yifan Shi",
      "Yiliang Xiong",
      "Ying He",
      "Yishi Piao",
      "Yisong Wang",
      "Yixuan Tan",
      "Yiyang Ma",
      "Yiyuan Liu",
      "Yongqiang Guo",
      "Yuan Ou",
      "Yuduan Wang",
      "Yue Gong",
      "Yuheng Zou",
      "Yujia He",
      "Yunfan Xiong",
      "Yuxiang Luo",
      "Yuxiang You",
      "Yuxuan Liu",
      "Yuyang Zhou",
      "Y. X. Zhu",
      "Yanhong Xu",
      "Yanping Huang",
      "Yaohui Li",
      "Yi Zheng",
      "Yuchen Zhu",
      "Yunxian Ma",
      "Ying Tang",
      "Yukun Zha",
      "Yuting Yan",
      "Z. Z. Ren",
      "Zehui Ren",
      "Zhangli Sha",
      "Zhe Fu",
      "Zhean Xu",
      "Zhenda Xie",
      "Zhengyan Zhang",
      "Zhewen Hao",
      "Zhicheng Ma",
      "Zhigang Yan",
      "Zhiyu Wu",
      "Zihui Gu",
      "Zijia Zhu",
      "Zijun Liu",
      "Zilin Li",
      "Ziwei Xie",
      "Ziyang Song",
      "Zizheng Pan",
      "Zhen Huang",
      "Zhipeng Xu",
      "Zhongyu Zhang",
      "Zhen Zhang"
    ],
    "abstract": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and\nDeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement\nlearning (RL) without supervised fine-tuning (SFT) as a preliminary step,\ndemonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero\nnaturally emerges with numerous powerful and intriguing reasoning behaviors.\nHowever, it encounters challenges such as poor readability, and language\nmixing. To address these issues and further enhance reasoning performance, we\nintroduce DeepSeek-R1, which incorporates multi-stage training and cold-start\ndata before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217\non reasoning tasks. To support the research community, we open-source\nDeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B,\n70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2501.12934v1",
    "title": "Correctness Assessment of Code Generated by Large Language Models Using Internal Representations",
    "publish_time": "20250122",
    "authors": [
      "Tuan-Dung Bui",
      "Thanh Trong Vu",
      "Thu-Trang Nguyen",
      "Son Nguyen",
      "Hieu Dinh Vo"
    ],
    "abstract": "Ensuring the correctness of code generated by Large Language Models (LLMs)\npresents a significant challenge in AI-driven software development. Existing\napproaches predominantly rely on black-box (closed-box) approaches that\nevaluate correctness post-generation, failing to utilize the rich insights\nembedded in the LLMs' internal states during code generation. In this paper, we\nintroduce OPENIA, a novel white-box (open-box) framework that leverages these\ninternal representations to assess the correctness of LLM-generated code.\nOPENIA systematically analyzes the intermediate states of representative\nopen-source LLMs specialized for code, including DeepSeek-Coder, CodeLlama, and\nMagicCoder, across diverse code generation benchmarks. Our empirical analysis\nreveals that these internal representations encode latent information, which\nstrongly correlates with the correctness of the generated code. Building on\nthese insights, OPENIA uses a white-box/open-box approach to make informed\npredictions about code correctness, offering significant advantages in\nadaptability and robustness over traditional classification-based methods and\nzero-shot approaches. Experimental results demonstrate that OPENIA consistently\noutperforms baseline models, achieving higher accuracy, precision, recall, and\nF1-Scores with up to a 2X improvement in standalone code generation and a 46%\nenhancement in repository-specific scenarios. By unlocking the potential of\nin-process signals, OPENIA paves the way for more proactive and efficient\nquality assurance mechanisms in LLM-assisted code generation.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2501.11223v3",
    "title": "Reasoning Language Models: A Blueprint",
    "publish_time": "20250120",
    "authors": [
      "Maciej Besta",
      "Julia Barth",
      "Eric Schreiber",
      "Ales Kubicek",
      "Afonso Catarino",
      "Robert Gerstenberger",
      "Piotr Nyczyk",
      "Patrick Iff",
      "Yueling Li",
      "Sam Houliston",
      "Tomasz Sternal",
      "Marcin Copik",
      "Grzegorz Kwaśniewski",
      "Jürgen Müller",
      "Łukasz Flis",
      "Hannes Eberhard",
      "Hubert Niewiadomski",
      "Torsten Hoefler"
    ],
    "abstract": "Reasoning language models (RLMs), also known as Large Reasoning Models\n(LRMs), such as OpenAI's o1 and o3, DeepSeek-V3, and Alibaba's QwQ, have\nredefined AI's problem-solving capabilities by extending LLMs with advanced\nreasoning mechanisms. Yet, their high costs, proprietary nature, and complex\narchitectures - uniquely combining Reinforcement Learning (RL), search\nheuristics, and LLMs - present accessibility and scalability challenges. To\naddress these, we propose a comprehensive blueprint that organizes RLM\ncomponents into a modular framework, based on a survey and analysis of all RLM\nworks. This blueprint incorporates diverse reasoning structures (chains, trees,\ngraphs, and nested forms), reasoning strategies (e.g., Monte Carlo Tree Search,\nBeam Search), RL concepts (policy, value models and others), supervision\nschemes (Outcome-Based and Process-Based Supervision), and other related\nconcepts (e.g., Test-Time Compute, Retrieval-Augmented Generation, agent\ntools). We also provide detailed mathematical formulations and algorithmic\nspecifications to simplify RLM implementation. By showing how schemes like\nLLaMA-Berry, QwQ, Journey Learning, and Graph of Thoughts fit as special cases,\nwe demonstrate the blueprint's versatility and unifying potential. To\nillustrate its utility, we introduce x1, a modular implementation for rapid RLM\nprototyping and experimentation. Using x1 and a literature review, we provide\nkey insights, such as multi-phase training for policy and value models, and the\nimportance of familiar training distributions. Finally, we discuss scalable RLM\ncloud deployments and we outline how RLMs can integrate with a broader LLM\necosystem. Our work demystifies RLM construction, democratizes advanced\nreasoning capabilities, and fosters innovation, aiming to mitigate the gap\nbetween \"rich AI\" and \"poor AI\" by lowering barriers to RLM design and\nexperimentation.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2501.04985v1",
    "title": "SpaLLM-Guard: Pairing SMS Spam Detection Using Open-source and Commercial LLMs",
    "publish_time": "20250109",
    "authors": [
      "Muhammad Salman",
      "Muhammad Ikram",
      "Nardine Basta",
      "Mohamed Ali Kaafar"
    ],
    "abstract": "The increasing threat of SMS spam, driven by evolving adversarial techniques\nand concept drift, calls for more robust and adaptive detection methods. In\nthis paper, we evaluate the potential of large language models (LLMs), both\nopen-source and commercial, for SMS spam detection, comparing their performance\nacross zero-shot, few-shot, fine-tuning, and chain-of-thought prompting\napproaches. Using a comprehensive dataset of SMS messages, we assess the spam\ndetection capabilities of prominent LLMs such as GPT-4, DeepSeek, LLAMA-2, and\nMixtral. Our findings reveal that while zero-shot learning provides\nconvenience, it is unreliable for effective spam detection. Few-shot learning,\nparticularly with carefully selected examples, improves detection but exhibits\nvariability across models. Fine-tuning emerges as the most effective strategy,\nwith Mixtral achieving 98.6% accuracy and a balanced false positive and false\nnegative rate below 2%, meeting the criteria for robust spam detection.\nFurthermore, we explore the resilience of these models to adversarial attacks,\nfinding that fine-tuning significantly enhances robustness against both\nperceptible and imperceptible manipulations. Lastly, we investigate the impact\nof concept drift and demonstrate that fine-tuned LLMs, especially when combined\nwith few-shot learning, can mitigate its effects, maintaining high performance\neven on evolving spam datasets. This study highlights the importance of\nfine-tuning and tailored learning strategies to deploy LLMs effectively for\nreal-world SMS spam detection",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2501.00135v2",
    "title": "GroverGPT: A Large Language Model with 8 Billion Parameters for Quantum Searching",
    "publish_time": "20241230",
    "authors": [
      "Haoran Wang",
      "Pingzhi Li",
      "Min Chen",
      "Jinglei Cheng",
      "Junyu Liu",
      "Tianlong Chen"
    ],
    "abstract": "Quantum computing is an exciting non-Von Neumann paradigm, offering provable\nspeedups over classical computing for specific problems. However, the practical\nlimits of classical simulatability for quantum circuits remain unclear,\nespecially with current noisy quantum devices. In this work, we explore the\npotential of leveraging Large Language Models (LLMs) to simulate the output of\na quantum Turing machine using Grover's quantum circuits, known to provide\nquadratic speedups over classical counterparts. To this end, we developed\nGroverGPT, a specialized model based on LLaMA's 8-billion-parameter\narchitecture, trained on over 15 trillion tokens. Unlike brute-force\nstate-vector simulations, which demand substantial computational resources,\nGroverGPT employs pattern recognition to approximate quantum search algorithms\nwithout explicitly representing quantum states. Analyzing 97K quantum search\ninstances, GroverGPT consistently outperformed OpenAI's GPT-4o (45\\% accuracy),\nachieving nearly 100\\% accuracy on 6- and 10-qubit datasets when trained on\n4-qubit or larger datasets. It also demonstrated strong generalization,\nsurpassing 95\\% accuracy for systems with over 20 qubits when trained on 3- to\n6-qubit data. Analysis indicates GroverGPT captures quantum features of\nGrover's search rather than classical patterns, supported by novel prompting\nstrategies to enhance performance. Although accuracy declines with increasing\nsystem size, these findings offer insights into the practical boundaries of\nclassical simulatability. This work suggests task-specific LLMs can surpass\ngeneral-purpose models like GPT-4o in quantum algorithm learning and serve as\npowerful tools for advancing quantum research.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2412.19437v1",
    "title": "DeepSeek-V3 Technical Report",
    "publish_time": "20241227",
    "authors": [
      "DeepSeek-AI",
      "Aixin Liu",
      "Bei Feng",
      "Bing Xue",
      "Bingxuan Wang",
      "Bochao Wu",
      "Chengda Lu",
      "Chenggang Zhao",
      "Chengqi Deng",
      "Chenyu Zhang",
      "Chong Ruan",
      "Damai Dai",
      "Daya Guo",
      "Dejian Yang",
      "Deli Chen",
      "Dongjie Ji",
      "Erhang Li",
      "Fangyun Lin",
      "Fucong Dai",
      "Fuli Luo",
      "Guangbo Hao",
      "Guanting Chen",
      "Guowei Li",
      "H. Zhang",
      "Han Bao",
      "Hanwei Xu",
      "Haocheng Wang",
      "Haowei Zhang",
      "Honghui Ding",
      "Huajian Xin",
      "Huazuo Gao",
      "Hui Li",
      "Hui Qu",
      "J. L. Cai",
      "Jian Liang",
      "Jianzhong Guo",
      "Jiaqi Ni",
      "Jiashi Li",
      "Jiawei Wang",
      "Jin Chen",
      "Jingchang Chen",
      "Jingyang Yuan",
      "Junjie Qiu",
      "Junlong Li",
      "Junxiao Song",
      "Kai Dong",
      "Kai Hu",
      "Kaige Gao",
      "Kang Guan",
      "Kexin Huang",
      "Kuai Yu",
      "Lean Wang",
      "Lecong Zhang",
      "Lei Xu",
      "Leyi Xia",
      "Liang Zhao",
      "Litong Wang",
      "Liyue Zhang",
      "Meng Li",
      "Miaojun Wang",
      "Mingchuan Zhang",
      "Minghua Zhang",
      "Minghui Tang",
      "Mingming Li",
      "Ning Tian",
      "Panpan Huang",
      "Peiyi Wang",
      "Peng Zhang",
      "Qiancheng Wang",
      "Qihao Zhu",
      "Qinyu Chen",
      "Qiushi Du",
      "R. J. Chen",
      "R. L. Jin",
      "Ruiqi Ge",
      "Ruisong Zhang",
      "Ruizhe Pan",
      "Runji Wang",
      "Runxin Xu",
      "Ruoyu Zhang",
      "Ruyi Chen",
      "S. S. Li",
      "Shanghao Lu",
      "Shangyan Zhou",
      "Shanhuang Chen",
      "Shaoqing Wu",
      "Shengfeng Ye",
      "Shengfeng Ye",
      "Shirong Ma",
      "Shiyu Wang",
      "Shuang Zhou",
      "Shuiping Yu",
      "Shunfeng Zhou",
      "Shuting Pan",
      "T. Wang",
      "Tao Yun",
      "Tian Pei",
      "Tianyu Sun",
      "W. L. Xiao",
      "Wangding Zeng",
      "Wanjia Zhao",
      "Wei An",
      "Wen Liu",
      "Wenfeng Liang",
      "Wenjun Gao",
      "Wenqin Yu",
      "Wentao Zhang",
      "X. Q. Li",
      "Xiangyue Jin",
      "Xianzu Wang",
      "Xiao Bi",
      "Xiaodong Liu",
      "Xiaohan Wang",
      "Xiaojin Shen",
      "Xiaokang Chen",
      "Xiaokang Zhang",
      "Xiaosha Chen",
      "Xiaotao Nie",
      "Xiaowen Sun",
      "Xiaoxiang Wang",
      "Xin Cheng",
      "Xin Liu",
      "Xin Xie",
      "Xingchao Liu",
      "Xingkai Yu",
      "Xinnan Song",
      "Xinxia Shan",
      "Xinyi Zhou",
      "Xinyu Yang",
      "Xinyuan Li",
      "Xuecheng Su",
      "Xuheng Lin",
      "Y. K. Li",
      "Y. Q. Wang",
      "Y. X. Wei",
      "Y. X. Zhu",
      "Yang Zhang",
      "Yanhong Xu",
      "Yanhong Xu",
      "Yanping Huang",
      "Yao Li",
      "Yao Zhao",
      "Yaofeng Sun",
      "Yaohui Li",
      "Yaohui Wang",
      "Yi Yu",
      "Yi Zheng",
      "Yichao Zhang",
      "Yifan Shi",
      "Yiliang Xiong",
      "Ying He",
      "Ying Tang",
      "Yishi Piao",
      "Yisong Wang",
      "Yixuan Tan",
      "Yiyang Ma",
      "Yiyuan Liu",
      "Yongqiang Guo",
      "Yu Wu",
      "Yuan Ou",
      "Yuchen Zhu",
      "Yuduan Wang",
      "Yue Gong",
      "Yuheng Zou",
      "Yujia He",
      "Yukun Zha",
      "Yunfan Xiong",
      "Yunxian Ma",
      "Yuting Yan",
      "Yuxiang Luo",
      "Yuxiang You",
      "Yuxuan Liu",
      "Yuyang Zhou",
      "Z. F. Wu",
      "Z. Z. Ren",
      "Zehui Ren",
      "Zhangli Sha",
      "Zhe Fu",
      "Zhean Xu",
      "Zhen Huang",
      "Zhen Zhang",
      "Zhenda Xie",
      "Zhengyan Zhang",
      "Zhewen Hao",
      "Zhibin Gou",
      "Zhicheng Ma",
      "Zhigang Yan",
      "Zhihong Shao",
      "Zhipeng Xu",
      "Zhiyu Wu",
      "Zhongyu Zhang",
      "Zhuoshu Li",
      "Zihui Gu",
      "Zijia Zhu",
      "Zijun Liu",
      "Zilin Li",
      "Ziwei Xie",
      "Ziyang Song",
      "Ziyi Gao",
      "Zizheng Pan"
    ],
    "abstract": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with\n671B total parameters with 37B activated for each token. To achieve efficient\ninference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent\nAttention (MLA) and DeepSeekMoE architectures, which were thoroughly validated\nin DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free\nstrategy for load balancing and sets a multi-token prediction training\nobjective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion\ndiverse and high-quality tokens, followed by Supervised Fine-Tuning and\nReinforcement Learning stages to fully harness its capabilities. Comprehensive\nevaluations reveal that DeepSeek-V3 outperforms other open-source models and\nachieves performance comparable to leading closed-source models. Despite its\nexcellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its\nfull training. In addition, its training process is remarkably stable.\nThroughout the entire training process, we did not experience any irrecoverable\nloss spikes or perform any rollbacks. The model checkpoints are available at\nhttps://github.com/deepseek-ai/DeepSeek-V3.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2412.17429v1",
    "title": "Condor: A Code Discriminator Integrating General Semantics with Code Details",
    "publish_time": "20241223",
    "authors": [
      "Qingyuan Liang",
      "Zhao Zhang",
      "Chen Liu",
      "Zeyu Sun",
      "Wenjie Zhang",
      "Yizhou Chen",
      "Zixiao Zhao",
      "Qi Luo",
      "Wentao Wang",
      "Yanjie Jiang",
      "Yingfei Xiong",
      "Lu Zhang"
    ],
    "abstract": "LLMs demonstrate significant potential across various software engineering\ntasks. However, they still face challenges in generating correct code on the\nfirst attempt when addressing complex requirements. Introducing a discriminator\nto select reliable outputs from multiple generated results is an effective way\nto enhance their reliability and stability. Currently, these discriminators\nfall into two categories: execution-based discriminators and\nnon-execution-based discriminators. Execution-based discriminators face\nflexibility challenges due to difficulties in obtaining test cases and security\nconcerns, while non-execution-based discriminators, although more flexible,\nstruggle to capture subtle differences in code details. To maintain flexibility\nwhile improving the model's ability to capture fine-grained code details, this\npaper proposes Condor. We first design contrastive learning to optimize the\ncode representations of the base model, enabling it to reflect differences in\ncode details. Then, we leverage intermediate data from the code modification\nprocess to further enrich the discriminator's training data, enhancing its\nability to discern code details. Experimental results indicate that on the\nsubtle code difference dataset (i.e., CodeNanoFix), Condor significantly\noutperforms other discriminators in discriminative performance: Condor (1.3B)\nimproves the discriminative F1 score of DeepSeek-Coder (1.3B) from 67% to 73%.\nIn discriminating LLM-generated outputs, Condor (1.3B) and Condor (110M) raise\nthe Pass@1 score of Meta-Llama-3.1-Instruct (70B) on the CodeNanoFix dataset\nfrom 52.64% to 62.63% and 59.64%, respectively. Moreover, Condor demonstrates\nstrong generalization capabilities on the MBPP and APPS datasets. For example,\nCondor (1.3B) improves the Pass@1 of Meta-Llama-3.1-Instruct (70B) on the APPS\ndataset by 147.05%.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2412.17874v1",
    "title": "Evaluating LLM Reasoning in the Operations Research Domain with ORQA",
    "publish_time": "20241222",
    "authors": [
      "Mahdi Mostajabdaveh",
      "Timothy T. Yu",
      "Samarendra Chandan Bindu Dash",
      "Rindranirina Ramamonjison",
      "Jabo Serge Byusa",
      "Giuseppe Carenini",
      "Zirui Zhou",
      "Yong Zhang"
    ],
    "abstract": "In this paper, we introduce and apply Operations Research Question Answering\n(ORQA), a new benchmark designed to assess the generalization capabilities of\nLarge Language Models (LLMs) in the specialized technical domain of Operations\nResearch (OR). This benchmark evaluates whether LLMs can emulate the knowledge\nand reasoning skills of OR experts when confronted with diverse and complex\noptimization problems. The dataset, developed by OR experts, features\nreal-world optimization problems that demand multistep reasoning to construct\ntheir mathematical models. Our evaluations of various open source LLMs, such as\nLLaMA 3.1, DeepSeek, and Mixtral, reveal their modest performance, highlighting\na gap in their ability to generalize to specialized technical domains. This\nwork contributes to the ongoing discourse on LLMs generalization capabilities,\noffering valuable insights for future research in this area. The dataset and\nevaluation code are publicly available.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2412.16589v1",
    "title": "Improving FIM Code Completions via Context & Curriculum Based Learning",
    "publish_time": "20241221",
    "authors": [
      "Hitesh Sagtani",
      "Rishabh Mehrotra",
      "Beyang Liu"
    ],
    "abstract": "Fill-in-the-Middle (FIM) models play a vital role in code completion tasks,\nleveraging both prefix and suffix context to provide more accurate and\ncontextually relevant suggestions. This paper presents approaches to improve\nFIM code completion while addressing the challenge of maintaining low latency\nfor real-time coding assistance. We enhance FIM code completion by\nincorporating context and curriculum examples in the training process. We\nidentify patterns where completion suggestions fail more frequently, revealing\ncomplexities that smaller language models struggle with. To address these\nchallenges, we develop a curriculum dataset by extracting hard-to-complete\npatterns from code repositories and generate context examples using semantic\nand static analysis tools (e.g. TSC compiler). We fine-tune various sized\nmodels, including StarCoder and DeepSeek, on this enhanced dataset. Our\nevaluation encompasses three key dimensions: the Santa Coder FIM task, the\nAmazon CCEval benchmark, and a new Multi-Line Infilling evaluation benchmark\nderived from SWE-bench. Comprehensive ablation studies across multiple model\nsizes reveal that while all fine-tuned models show improvements, the\nperformance gains are more pronounced for smaller parameter models and\nincorporating difficult-to-complete examples, as part of curriculum learning,\nimproves the code completion performance. This finding is particularly\nsignificant given the latency constraints of code completion tasks. While\nlarger models like GPT and Claude perform well in multi-line completions but\nare prohibitively challenging to use given high latency, and our fine-tuned\nmodels achieve a balance between performance and latency. Finally, we validate\nour approach through online A/B testing, demonstrating tangible improvements in\nCompletion Acceptance Rate (CAR) and Completion Persistence Rate (CPR), with\nzero latency impact.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2412.10302v1",
    "title": "DeepSeek-VL2: Mixture-of-Experts Vision-Language Models for Advanced Multimodal Understanding",
    "publish_time": "20241213",
    "authors": [
      "Zhiyu Wu",
      "Xiaokang Chen",
      "Zizheng Pan",
      "Xingchao Liu",
      "Wen Liu",
      "Damai Dai",
      "Huazuo Gao",
      "Yiyang Ma",
      "Chengyue Wu",
      "Bingxuan Wang",
      "Zhenda Xie",
      "Yu Wu",
      "Kai Hu",
      "Jiawei Wang",
      "Yaofeng Sun",
      "Yukun Li",
      "Yishi Piao",
      "Kang Guan",
      "Aixin Liu",
      "Xin Xie",
      "Yuxiang You",
      "Kai Dong",
      "Xingkai Yu",
      "Haowei Zhang",
      "Liang Zhao",
      "Yisong Wang",
      "Chong Ruan"
    ],
    "abstract": "We present DeepSeek-VL2, an advanced series of large Mixture-of-Experts (MoE)\nVision-Language Models that significantly improves upon its predecessor,\nDeepSeek-VL, through two key major upgrades. For the vision component, we\nincorporate a dynamic tiling vision encoding strategy designed for processing\nhigh-resolution images with different aspect ratios. For the language\ncomponent, we leverage DeepSeekMoE models with the Multi-head Latent Attention\nmechanism, which compresses Key-Value cache into latent vectors, to enable\nefficient inference and high throughput. Trained on an improved vision-language\ndataset, DeepSeek-VL2 demonstrates superior capabilities across various tasks,\nincluding but not limited to visual question answering, optical character\nrecognition, document/table/chart understanding, and visual grounding. Our\nmodel series is composed of three variants: DeepSeek-VL2-Tiny,\nDeepSeek-VL2-Small and DeepSeek-VL2, with 1.0B, 2.8B and 4.5B activated\nparameters respectively. DeepSeek-VL2 achieves competitive or state-of-the-art\nperformance with similar or fewer activated parameters compared to existing\nopen-source dense and MoE-based models. Codes and pre-trained models are\npublicly accessible at https://github.com/deepseek-ai/DeepSeek-VL2.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2412.00251v1",
    "title": "Fine Tuning Large Language Models to Deliver CBT for Depression",
    "publish_time": "20241129",
    "authors": [
      "Talha Tahir"
    ],
    "abstract": "Cognitive Behavioral Therapy (CBT) is a well-established, evidence-based\ntreatment for Major Depressive Disorder. Unfortunately, there exist significant\nbarriers to individuals accessing CBT, including cost, scarcity of therapists\nand stigma. This study explores the feasibility of fine-tuning small open\nweight large language models (LLMs) to deliver CBT for depression. Using 58\nsets of synthetic CBT transcripts generated by the Nous Research fine-tune of\nLlama 3.1 405b, we fine-tuned three models: Mistral 7b v0.3, Qwen 2.5 7b, and\nLlama 3.1 8b. CBT fidelity was evaluated through a modified Cognitive Therapy\nRating Scale (CTRS). All fine-tuned models were compared against each other, as\nwell as their instruct-tuned variants. Simulated patient transcripts were\ngenerated for the purpose of evaluating model performance, with the instruct\nand CBT-tuned models acting as the therapist and DeepSeek-V2.5 acting as the\npatient. These simulated transcripts were evaluated on a modified CTRS by\nGemini 1.5 Pro-002. Our findings demonstrated that the CBT-tuned models\nsignificantly outperformed their instruct-tuned counterparts, with an average\nimprovement of 11.33 points (p < 0.001) on total CTRS score. Llama 3.1 8b had\nthe strongest performance (mean CTRS score 67.86 +/- 7.24), followed by Qwen\n2.5 7b (64.28 +/- 9.55) and Mistral 7b v0.3 (64.17 +/- 9.79), with these\ndifferences between models being statistically significant. The CBT-tuned\nmodels were competent in implementing core CBT techniques and providing\nempathetic responses, however, there were limitations observed in agenda\nadherence, exploration depth and long-context coherence. This study establishes\nthat CBT specific fine-tuning can effectively encode therapeutic competencies\nin small LLMs, though significant technical and ethical considerations must be\nresolved prior to clinical deployment.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2411.19943v3",
    "title": "Critical Tokens Matter: Token-Level Contrastive Estimation Enhances LLM's Reasoning Capability",
    "publish_time": "20241129",
    "authors": [
      "Zicheng Lin",
      "Tian Liang",
      "Jiahao Xu",
      "Qiuzhi Lin",
      "Xing Wang",
      "Ruilin Luo",
      "Chufan Shi",
      "Siheng Li",
      "Yujiu Yang",
      "Zhaopeng Tu"
    ],
    "abstract": "Mathematical reasoning tasks pose significant challenges for large language\nmodels (LLMs) because they require precise logical deduction and sequence\nanalysis. In this work, we introduce the concept of critical tokens -- elements\nwithin reasoning trajectories that significantly influence incorrect outcomes.\nWe present a novel framework for identifying these tokens through rollout\nsampling and demonstrate their substantial divergence from traditional error\ntokens. Through extensive experiments on datasets such as GSM8K and MATH500, we\nshow that identifying and replacing critical tokens significantly improves\nmodel accuracy. We propose an efficient methodology for pinpointing these\ntokens in large-scale datasets using contrastive estimation and extend this\nframework to enhance model training processes with direct preference\noptimization (DPO). Experimental results on GSM8K and MATH500 benchmarks with\nthe widely used models Llama-3 (8B and 70B) and Deepseek-math (7B) demonstrate\nthe effectiveness of the proposed approach, cDPO. Our results underscore the\npotential of leveraging critical tokens to reduce errors in reasoning tasks,\nadvancing the development of AI systems capable of robust logical deduction.\nOur code, annotated datasets, and trained models are available at\nhttps://github.com/chenzhiling9954/Critical-Tokens-Matter to support and\nencourage future research in this promising field.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2411.18564v2",
    "title": "Dspy-based Neural-Symbolic Pipeline to Enhance Spatial Reasoning in LLMs",
    "publish_time": "20241127",
    "authors": [
      "Rong Wang",
      "Kun Sun",
      "Jonas Kuhn"
    ],
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable capabilities across\nvarious tasks, yet they often struggle with spatial reasoning. This paper\npresents a novel neural-symbolic framework that enhances LLMs' spatial\nreasoning abilities through iterative feedback between LLMs and Answer Set\nProgramming (ASP). We evaluate our approach on two benchmark datasets: StepGame\nand SparQA, implementing three distinct strategies: (1) direct prompting\nbaseline, (2) Facts+Rules prompting, and (3) DSPy-based LLM+ASP pipeline with\niterative refinement. Our experimental results demonstrate that the LLM+ASP\npipeline significantly outperforms baseline methods, achieving an average 82%\naccuracy on StepGame and 69% on SparQA, marking improvements of 40-50% and\n8-15% respectively over direct prompting. The success stems from three key\ninnovations: (1) effective separation of semantic parsing and logical reasoning\nthrough a modular pipeline, (2) iterative feedback mechanism between LLMs and\nASP solvers that improves program rate, and (3) robust error handling that\naddresses parsing, grounding, and solving failures. Additionally, we propose\nFacts+Rules as a lightweight alternative that achieves comparable performance\non complex SparQA dataset, while reducing computational overhead.Our analysis\nacross different LLM architectures (Deepseek, Llama3-70B, GPT-4.0 mini)\ndemonstrates the framework's generalizability and provides insights into the\ntrade-offs between implementation complexity and reasoning capability,\ncontributing to the development of more interpretable and reliable AI systems.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2411.17426v3",
    "title": "CLOVER: Cross-Layer Orthogonal Vectors Pruning and Fine-Tuning",
    "publish_time": "20241126",
    "authors": [
      "Fanxu Meng",
      "Pingzhi Tang",
      "Fan jiang",
      "Muhan Zhang"
    ],
    "abstract": "Decoder-only models generate tokens autoregressively by caching key/value\nvectors, but as the cache grows, inference becomes memory-bound. To address\nthis issue, we introduce CLOVER (Cross-Layer Orthogonal Vectors), a novel\napproach that treats pairs of attention layers as a set of low-rank\ndecompositions. CLOVER applies Singular Value Decomposition (SVD) to the \\( Q\n\\)-\\( K \\) and \\( V \\)-\\( O \\) pairs within each attention head. The resulting\nsingular values can either guide pruning or serve as trainable parameters for\nefficient fine-tuning of all orthogonal vectors. After pruning or fine-tuning,\nthese values are reintegrated into the model without increasing its parameter\ncount. We apply CLOVER to various models, including GPT-2 XL, DeepSeek-V2-Lite,\nWhisper-Large-v3, Stable Diffusion XL, and LLaMA-3.2-11B-Vision. Our results\ndemonstrate that CLOVER significantly improves pruning efficiency. For\ninstance, the perplexity of pruning 70\\% of the \\( Q \\)-\\( K \\) pairs in GPT-2\nXL is similar to that of pruning just 8\\% with vanilla methods. Fine-tuning the\nsingular values further results in a full-rank update, outperforming\nstate-of-the-art methods (LoRA, DoRA, HiRA, and PiSSA) by 7.6\\%, 5.5\\%, 3.8\\%,\nand 0.7\\%, respectively, on eight commonsense tasks for LLaMA-2 7B.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2411.16345v1",
    "title": "Preference Optimization for Reasoning with Pseudo Feedback",
    "publish_time": "20241125",
    "authors": [
      "Fangkai Jiao",
      "Geyang Guo",
      "Xingxing Zhang",
      "Nancy F. Chen",
      "Shafiq Joty",
      "Furu Wei"
    ],
    "abstract": "Preference optimization techniques, such as Direct Preference Optimization\n(DPO), are frequently employed to enhance the reasoning capabilities of large\nlanguage models (LLMs) in domains like mathematical reasoning and coding,\ntypically following supervised fine-tuning. These methods rely on high-quality\nlabels for reasoning tasks to generate preference pairs; however, the\navailability of reasoning datasets with human-verified labels is limited. In\nthis study, we introduce a novel approach to generate pseudo feedback for\nreasoning tasks by framing the labeling of solutions to reason problems as an\nevaluation against associated test cases. We explore two forms of pseudo\nfeedback based on test cases: one generated by frontier LLMs and the other by\nextending self-consistency to multi-test-case. We conduct experiments on both\nmathematical reasoning and coding tasks using pseudo feedback for preference\noptimization, and observe improvements across both tasks. Specifically, using\nMathstral-7B as our base model, we improve MATH results from 58.3 to 68.6,\nsurpassing both NuminaMath-72B and GPT-4-Turbo-1106-preview. In GSM8K and\nCollege Math, our scores increase from 85.6 to 90.3 and from 34.3 to 42.3,\nrespectively. Building on Deepseek-coder-7B-v1.5, we achieve a score of 24.6 on\nLiveCodeBench (from 21.1), surpassing Claude-3-Haiku.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2411.15203v1",
    "title": "Multimodal large language model for wheat breeding: a new exploration of smart breeding",
    "publish_time": "20241120",
    "authors": [
      "Guofeng Yang",
      "Yu Li",
      "Yong He",
      "Zhenjiang Zhou",
      "Lingzhen Ye",
      "Hui Fang",
      "Yiqi Luo",
      "Xuping Feng"
    ],
    "abstract": "UAV remote sensing technology has become a key technology in crop breeding,\nwhich can achieve high-throughput and non-destructive collection of crop\nphenotyping data. However, the multidisciplinary nature of breeding has brought\ntechnical barriers and efficiency challenges to knowledge mining. Therefore, it\nis important to develop a smart breeding goal tool to mine cross-domain\nmultimodal data. Based on different pre-trained open-source multimodal large\nlanguage models (MLLMs) (e.g., Qwen-VL, InternVL, Deepseek-VL), this study used\nsupervised fine-tuning (SFT), retrieval-augmented generation (RAG), and\nreinforcement learning from human feedback (RLHF) technologies to inject\ncross-domain knowledge into MLLMs, thereby constructing multiple multimodal\nlarge language models for wheat breeding (WBLMs). The above WBLMs were\nevaluated using the newly created evaluation benchmark in this study. The\nresults showed that the WBLM constructed using SFT, RAG and RLHF technologies\nand InternVL2-8B has leading performance. Then, subsequent experiments were\nconducted using the WBLM. Ablation experiments indicated that the combination\nof SFT, RAG, and RLHF technologies can improve the overall generation\nperformance, enhance the generated quality, balance the timeliness and\nadaptability of the generated answer, and reduce hallucinations and biases. The\nWBLM performed best in wheat yield prediction using cross-domain data (remote\nsensing, phenotyping, weather, germplasm) simultaneously, with R2 and RMSE of\n0.821 and 489.254 kg/ha, respectively. Furthermore, the WBLM can generate\nprofessional decision support answers for phenotyping estimation, environmental\nstress assessment, target germplasm screening, cultivation technique\nrecommendation, and seed price query tasks.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2411.10599v1",
    "title": "Generating Energy-efficient code with LLMs",
    "publish_time": "20241115",
    "authors": [
      "Tom Cappendijk",
      "Pepijn de Reus",
      "Ana Oprescu"
    ],
    "abstract": "The increasing electricity demands of personal computers, communication\nnetworks, and data centers contribute to higher atmospheric greenhouse gas\nemissions, which in turn lead to global warming and climate change. Therefore\nthe energy consumption of code must be minimized. Code can be generated by\nlarge language models. We look at the influence of prompt modification on the\nenergy consumption of the code generated. We use three different Python code\nproblems of varying difficulty levels. Prompt modification is done by adding\nthe sentence ``Give me an energy-optimized solution for this problem'' or by\nusing two Python coding best practices. The large language models used are\nCodeLlama-70b, CodeLlama-70b-Instruct, CodeLlama-70b-Python,\nDeepSeek-Coder-33b-base, and DeepSeek-Coder-33b-instruct. We find a decrease in\nenergy consumption for a specific combination of prompt optimization, LLM, and\nPython code problem. However, no single optimization prompt consistently\ndecreases energy consumption for the same LLM across the different Python code\nproblems.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2411.06767v1",
    "title": "PDC & DM-SFT: A Road for LLM SQL Bug-Fix Enhancing",
    "publish_time": "20241111",
    "authors": [
      "Yiwen Duan",
      "Yonghong Yu",
      "Xiaoming Zhao",
      "Yichang Wu",
      "Wenbo Liu"
    ],
    "abstract": "Code Large Language Models (Code LLMs), such as Code llama and\nDeepSeek-Coder, have demonstrated exceptional performance in the code\ngeneration tasks. However, most existing models focus on the abilities of\ngenerating correct code, but often struggle with bug repair. We introduce a\nsuit of methods to enhance LLM's SQL bug-fixing abilities. The methods are\nmainly consisted of two parts: A Progressive Dataset Construction (PDC) from\nscratch and Dynamic Mask Supervised Fine-tuning (DM-SFT). PDC proposes two data\nexpansion methods from the perspectives of breadth first and depth first\nrespectively. DM-SFT introduces an efficient bug-fixing supervised learning\napproach, which effectively reduce the total training steps and mitigate the\n\"disorientation\" in SQL code bug-fixing training. In our evaluation, the code\nLLM models trained with two methods have exceeds all current best performing\nmodel which size is much larger.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2411.01016v1",
    "title": "MoE-I$^2$: Compressing Mixture of Experts Models through Inter-Expert Pruning and Intra-Expert Low-Rank Decomposition",
    "publish_time": "20241101",
    "authors": [
      "Cheng Yang",
      "Yang Sui",
      "Jinqi Xiao",
      "Lingyi Huang",
      "Yu Gong",
      "Yuanlin Duan",
      "Wenqi Jia",
      "Miao Yin",
      "Yu Cheng",
      "Bo Yuan"
    ],
    "abstract": "The emergence of Mixture of Experts (MoE) LLMs has significantly advanced the\ndevelopment of language models. Compared to traditional LLMs, MoE LLMs\noutperform traditional LLMs by achieving higher performance with considerably\nfewer activated parameters. Despite this efficiency, their enormous parameter\nsize still leads to high deployment costs. In this paper, we introduce a\ntwo-stage compression method tailored for MoE to reduce the model size and\ndecrease the computational cost. First, in the inter-expert pruning stage, we\nanalyze the importance of each layer and propose the Layer-wise Genetic Search\nand Block-wise KT-Reception Field with the non-uniform pruning ratio to prune\nthe individual expert. Second, in the intra-expert decomposition stage, we\napply the low-rank decomposition to further compress the parameters within the\nremaining experts. Extensive experiments on Qwen1.5-MoE-A2.7B,\nDeepSeek-V2-Lite, and Mixtral-8$\\times$7B demonstrate that our proposed methods\ncan both reduce the model size and enhance inference efficiency while\nmaintaining performance in various zero-shot tasks. The code will be available\nat \\url{https://github.com/xiaochengsky/MoEI-2.git}",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2411.00585v1",
    "title": "Benchmarking Bias in Large Language Models during Role-Playing",
    "publish_time": "20241101",
    "authors": [
      "Xinyue Li",
      "Zhenpeng Chen",
      "Jie M. Zhang",
      "Yiling Lou",
      "Tianlin Li",
      "Weisong Sun",
      "Yang Liu",
      "Xuanzhe Liu"
    ],
    "abstract": "Large Language Models (LLMs) have become foundational in modern\nlanguage-driven applications, profoundly influencing daily life. A critical\ntechnique in leveraging their potential is role-playing, where LLMs simulate\ndiverse roles to enhance their real-world utility. However, while research has\nhighlighted the presence of social biases in LLM outputs, it remains unclear\nwhether and to what extent these biases emerge during role-playing scenarios.\nIn this paper, we introduce BiasLens, a fairness testing framework designed to\nsystematically expose biases in LLMs during role-playing. Our approach uses\nLLMs to generate 550 social roles across a comprehensive set of 11 demographic\nattributes, producing 33,000 role-specific questions targeting various forms of\nbias. These questions, spanning Yes/No, multiple-choice, and open-ended\nformats, are designed to prompt LLMs to adopt specific roles and respond\naccordingly. We employ a combination of rule-based and LLM-based strategies to\nidentify biased responses, rigorously validated through human evaluation. Using\nthe generated questions as the benchmark, we conduct extensive evaluations of\nsix advanced LLMs released by OpenAI, Mistral AI, Meta, Alibaba, and DeepSeek.\nOur benchmark reveals 72,716 biased responses across the studied LLMs, with\nindividual models yielding between 7,754 and 16,963 biased responses,\nunderscoring the prevalence of bias in role-playing contexts. To support future\nresearch, we have publicly released the benchmark, along with all scripts and\nexperimental results.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2411.00073v2",
    "title": "RSL-SQL: Robust Schema Linking in Text-to-SQL Generation",
    "publish_time": "20241031",
    "authors": [
      "Zhenbiao Cao",
      "Yuanlei Zheng",
      "Zhihao Fan",
      "Xiaojin Zhang",
      "Wei Chen",
      "Xiang Bai"
    ],
    "abstract": "Text-to-SQL generation aims to translate natural language questions into SQL\nstatements. In Text-to-SQL based on large language models, schema linking is a\nwidely adopted strategy to streamline the input for LLMs by selecting only\nrelevant schema elements, therefore reducing noise and computational overhead.\nHowever, schema linking faces risks that require caution, including the\npotential omission of necessary elements and disruption of database structural\nintegrity. To address these challenges, we propose a novel framework called\nRSL-SQL that combines bidirectional schema linking, contextual information\naugmentation, binary selection strategy, and multi-turn self-correction. We\nimprove the recall of pattern linking using forward and backward pruning\nmethods, achieving a strict recall of 94% while reducing the number of input\ncolumns by 83%. Furthermore, it hedges the risk by voting between a full mode\nand a simplified mode enhanced with contextual information. Experiments on the\nBIRD and Spider benchmarks demonstrate that our approach achieves SOTA\nexecution accuracy among open-source solutions, with 67.2% on BIRD and 87.9% on\nSpider using GPT-4o. Furthermore, our approach outperforms a series of GPT-4\nbased Text-to-SQL systems when adopting DeepSeek (much cheaper) with same\nintact prompts. Extensive analysis and ablation studies confirm the\neffectiveness of each component in our framework. The codes are available at\nhttps://github.com/Laqcce-cao/RSL-SQL.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2410.22821v1",
    "title": "EvoCodeBench: An Evolving Code Generation Benchmark with Domain-Specific Evaluations",
    "publish_time": "20241030",
    "authors": [
      "Jia Li",
      "Ge Li",
      "Xuanming Zhang",
      "Yunfei Zhao",
      "Yihong Dong",
      "Zhi Jin",
      "Binhua Li",
      "Fei Huang",
      "Yongbin Li"
    ],
    "abstract": "How to evaluate Large Language Models (LLMs) in code generation remains an\nopen question. Existing benchmarks have two limitations - data leakage and lack\nof domain-specific evaluation. The former hurts the fairness of benchmarks, and\nthe latter hinders practitioners from selecting superior LLMs for specific\nprogramming domains. To address these two limitations, we propose a new\nbenchmark - EvoCodeBench, which has the following advances: (1) Evolving data.\nEvoCodeBench will be dynamically updated every period (e.g., 6 months) to avoid\ndata leakage. This paper releases the first version - EvoCodeBench-2403,\ncontaining 275 samples from 25 repositories. (2) A domain taxonomy and domain\nlabels. Based on the statistics of open-source communities, we design a\nprogramming domain taxonomy consisting of 10 popular domains. Based on the\ntaxonomy, we annotate each sample in EvoCodeBench with a domain label. (3)\nDomain-specific evaluations. Besides the Pass@k, we compute the Domain-Specific\nImprovement (DSI) and define LLMs' comfort and strange domains. These\nevaluations help practitioners select superior LLMs in specific domains and\ndiscover the shortcomings of existing LLMs. We evaluate 8 popular LLMs (e.g.,\ngpt-4, DeepSeek Coder) on EvoCodeBench and summarize some insights.\nEvoCodeBench reveals the actual abilities of these LLMs in real-world\nrepositories. For example, the highest Pass@1 of gpt-4 on EvoCodeBench-2403 is\nonly 20.74%. Besides, we evaluate LLMs in different domains and discover their\ncomfort and strange domains. For example, gpt-4 performs best in most domains\nbut falls behind others in the Internet domain. StarCoder 2-15B unexpectedly\nperforms well in the Database domain and even outperforms 33B LLMs.\nEvoCodeBench has been released.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2410.15966v1",
    "title": "Self-Explained Keywords Empower Large Language Models for Code Generation",
    "publish_time": "20241021",
    "authors": [
      "Lishui Fan",
      "Mouxiang Chen",
      "Zhongxin Liu"
    ],
    "abstract": "Large language models (LLMs) have achieved impressive performance in code\ngeneration. However, due to the long-tail distribution of LLMs' training data,\nlow-frequency terms are typically underrepresented in the training process.\nConsequently, LLMs often misunderstand or overlook problem-specific,\nlow-frequency keywords during code generation, compromising the accuracy of the\ngenerated code. To address this, we propose a novel technique named\nSEK(\\textbf{S}elf-\\textbf{E}xplained \\textbf{K}eywords), which empowers an LLM\nfor better code generation by extracting and explaining the key terms in the\nproblem description with the LLM itself and ranking them based on frequency.\nComprehensive experiments across three benchmarks, i.e., HumanEval(+), MBPP(+),\nand APPS, with five representative LLMs, show that SEK can significantly\nimprove LLMs in code generation, yielding substantial and consistent gains. For\ninstance, SEK improves the Pass@1 of DeepSeek-Coder-V2-Instruct from 85.4\\% to\n93.3\\% on the Humaneval benchmark. Further analysis confirms that SEK enables\nthe LLMs to shift their attention from low-frequency keywords to their\ncorresponding high-frequency counterparts.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2410.14321v1",
    "title": "From Solitary Directives to Interactive Encouragement! LLM Secure Code Generation by Natural Language Prompting",
    "publish_time": "20241018",
    "authors": [
      "Shigang Liu",
      "Bushra Sabir",
      "Seung Ick Jang",
      "Yuval Kansal",
      "Yansong Gao",
      "Kristen Moore",
      "Alsharif Abuadbba",
      "Surya Nepal"
    ],
    "abstract": "Large Language Models (LLMs) have shown remarkable potential in code\ngeneration, making them increasingly important in the field. However, the\nsecurity issues of generated code have not been fully addressed, and the\nusability of LLMs in code generation still requires further exploration.\n  This work introduces SecCode, a framework that leverages an innovative\ninteractive encouragement prompting (EP) technique for secure code generation\nwith \\textit{only NL} prompts. This approach ensures that the prompts can be\neasily shared and understood by general users. SecCode functions through three\nstages: 1) Code Generation using NL Prompts; 2) Code Vulnerability Detection\nand Fixing, utilising our proposed encouragement prompting; 3) Vulnerability\nCross-Checking and Code Security Refinement. These stages are executed in\nmultiple interactive iterations to progressively enhance security. By using\nboth proprietary LLMs (i.e., GPT-3.5 Turbo, GPT-4 and GPT-4o) and open-source\nLLMs (i.e., Llama 3.1 8B Instruct, DeepSeek Coder V2 Lite Instruct) evaluated\non three benchmark datasets, extensive experimental results show that our\nproposed SecCode greatly outperforms compared baselines, generating secure code\nwith a high vulnerability correction rate. For example, SecCode exhibits a high\nfix success rate of over 76\\% after running 5 automated EP interactive\niterations and over 89\\% after running 10 automated EP interactive iterations.\nTo the best of our knowledge, this work is the first to formulate secure code\ngeneration with NL prompts only. We have open-sourced our code and encourage\nthe community to focus on secure code generation.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2410.10209v2",
    "title": "Effi-Code: Unleashing Code Efficiency in Language Models",
    "publish_time": "20241014",
    "authors": [
      "Dong Huang",
      "Guangtao Zeng",
      "Jianbo Dai",
      "Meng Luo",
      "Han Weng",
      "Yuhao Qing",
      "Heming Cui",
      "Zhijiang Guo",
      "Jie M. Zhang"
    ],
    "abstract": "As the use of large language models (LLMs) for code generation becomes more\nprevalent in software development, it is critical to enhance both the\nefficiency and correctness of the generated code. Existing methods and models\nprimarily focus on the correctness of LLM-generated code, ignoring efficiency.\nIn this work, we present Effi-Code, an approach to enhancing code generation in\nLLMs that can improve both efficiency and correctness. We introduce a\nSelf-Optimization process based on Overhead Profiling that leverages\nopen-source LLMs to generate a high-quality dataset of correct and efficient\ncode samples. This dataset is then used to fine-tune various LLMs. Our method\ninvolves the iterative refinement of generated code, guided by runtime\nperformance metrics and correctness checks. Extensive experiments demonstrate\nthat models fine-tuned on the Effi-Code show significant improvements in both\ncode correctness and efficiency across task types. For example, the pass@1 of\nDeepSeek-Coder-6.7B-Instruct generated code increases from \\textbf{43.3\\%} to\n\\textbf{76.8\\%}, and the average execution time for the same correct tasks\ndecreases by \\textbf{30.5\\%}. Effi-Code offers a scalable and generalizable\napproach to improving code generation in AI systems, with potential\napplications in software development, algorithm design, and computational\nproblem-solving. The source code of Effi-Code was released in\n\\url{https://github.com/huangd1999/Effi-Code}.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2410.08632v1",
    "title": "Words as Beacons: Guiding RL Agents with High-Level Language Prompts",
    "publish_time": "20241011",
    "authors": [
      "Unai Ruiz-Gonzalez",
      "Alain Andres",
      "Pedro G. Bascoy",
      "Javier Del Ser"
    ],
    "abstract": "Sparse reward environments in reinforcement learning (RL) pose significant\nchallenges for exploration, often leading to inefficient or incomplete learning\nprocesses. To tackle this issue, this work proposes a teacher-student RL\nframework that leverages Large Language Models (LLMs) as \"teachers\" to guide\nthe agent's learning process by decomposing complex tasks into subgoals. Due to\ntheir inherent capability to understand RL environments based on a textual\ndescription of structure and purpose, LLMs can provide subgoals to accomplish\nthe task defined for the environment in a similar fashion to how a human would\ndo. In doing so, three types of subgoals are proposed: positional targets\nrelative to the agent, object representations, and language-based instructions\ngenerated directly by the LLM. More importantly, we show that it is possible to\nquery the LLM only during the training phase, enabling agents to operate within\nthe environment without any LLM intervention. We assess the performance of this\nproposed framework by evaluating three state-of-the-art open-source LLMs\n(Llama, DeepSeek, Qwen) eliciting subgoals across various procedurally\ngenerated environment of the MiniGrid benchmark. Experimental results\ndemonstrate that this curriculum-based approach accelerates learning and\nenhances exploration in complex tasks, achieving up to 30 to 200 times faster\nconvergence in training steps compared to recent baselines designed for sparse\nreward environments.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2410.06667v2",
    "title": "Large Language Models as Code Executors: An Exploratory Study",
    "publish_time": "20241009",
    "authors": [
      "Chenyang Lyu",
      "Lecheng Yan",
      "Rui Xing",
      "Wenxi Li",
      "Younes Samih",
      "Tianbo Ji",
      "Longyue Wang"
    ],
    "abstract": "The capabilities of Large Language Models (LLMs) have significantly evolved,\nextending from natural language processing to complex tasks like code\nunderstanding and generation. We expand the scope of LLMs' capabilities to a\nbroader context, using LLMs to execute code snippets to obtain the output. This\npaper pioneers the exploration of LLMs as code executors, where code snippets\nare directly fed to the models for execution, and outputs are returned. We are\nthe first to comprehensively examine this feasibility across various LLMs,\nincluding OpenAI's o1, GPT-4o, GPT-3.5, DeepSeek, and Qwen-Coder. Notably, the\no1 model achieved over 90% accuracy in code execution, while others\ndemonstrated lower accuracy levels. Furthermore, we introduce an Iterative\nInstruction Prompting (IIP) technique that processes code snippets line by\nline, enhancing the accuracy of weaker models by an average of 7.22% (with the\nhighest improvement of 18.96%) and an absolute average improvement of 3.86%\nagainst CoT prompting (with the highest improvement of 19.46%). Our study not\nonly highlights the transformative potential of LLMs in coding but also lays\nthe groundwork for future advancements in automated programming and the\ncompletion of complex tasks.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2410.03234v1",
    "title": "Showing LLM-Generated Code Selectively Based on Confidence of LLMs",
    "publish_time": "20241004",
    "authors": [
      "Jia Li",
      "Yuqi Zhu",
      "Yongmin Li",
      "Ge Li",
      "Zhi Jin"
    ],
    "abstract": "Large Language Models (LLMs) have shown impressive abilities in code\ngeneration, but they may generate erroneous programs. Reading a program takes\nten times longer than writing it. Showing these erroneous programs to\ndevelopers will waste developers' energies and introduce security risks to\nsoftware.\n  To address the above limitations, we propose HonestCoder, a novel LLM-based\ncode generation approach. HonestCoder selectively shows the generated programs\nto developers based on LLMs' confidence. The confidence provides valuable\ninsights into the correctness of generated programs. To achieve this goal, we\npropose a novel approach to estimate LLMs' confidence in code generation. It\nestimates confidence by measuring the multi-modal similarity between\nLLMs-generated programs.\n  We collect and release a multilingual benchmark named TruthCodeBench, which\nconsists of 2,265 samples and covers two popular programming languages (i.e.,\nPython and Java). We apply HonestCoder to four popular LLMs (e.g.,\nDeepSeek-Coder and Code Llama) and evaluate it on TruthCodeBench. Based on the\nexperiments, we obtain the following insights. (1) HonestCoder can effectively\nestimate LLMs' confidence and accurately determine the correctness of generated\nprograms. For example, HonestCoder outperforms the state-of-the-art baseline by\n27.79% in AUROC and 63.74% in AUCPR. (2) HonestCoder can decrease the number of\nerroneous programs shown to developers. Compared to eight baselines, it can\nshow more correct programs and fewer erroneous programs to developers. (3)\nCompared to showing code indiscriminately, HonestCoder only adds slight time\noverhead (approximately 0.4 seconds per requirement). (4) We discuss future\ndirections to facilitate the application of LLMs in software development. We\nhope this work can motivate broad discussions about measuring the reliability\nof LLMs' outputs in performing code-related tasks.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2410.00359v1",
    "title": "Self-controller: Controlling LLMs with Multi-round Step-by-step Self-awareness",
    "publish_time": "20241001",
    "authors": [
      "Xiao Peng",
      "Xufan Geng"
    ],
    "abstract": "The applications of large language models (LLMs) have been widely spread\nacross all domains. However, the basic abilities such as the controllability of\nLLMs are still limited. To address this, we propose \"Self-controller\", a novel\nagentic framework bringing self-awareness into LLMs' reasoning logic. The core\nidea of this work is to maintain states based on the LLM's response, letting\nthe LLM become self-aware of current status and think step by step in a\nmulti-round chain-of-thought paradigm. Our experiment on the state of textual\nlength has shown the controllability and effectiveness of the Self-controller.\nWe further implement a binary search algorithm to accelerate the generation\nprocess based on the linearity and monotonicity of the textual length state.\nAnother advantage of the Self-controller comes with DeepSeek's Context Caching\ntechnology, which significantly saves computational token consumption when a\ncluster of conversations shares the same prefix of context. Theoretically, we\nprove that in this scenario the extra time complexity is $O(c \\log n)$. Results\nof the back-of-the-envelope estimation suggest that the token consumption of\nour method is no more than twice as much as that of the trivial single-round\ngeneration. Furthermore, our ablation study on word constraints demonstrates\nthe Self-controller's consistent controllability across all foundation models.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2409.16751v2",
    "title": "E-SQL: Direct Schema Linking via Question Enrichment in Text-to-SQL",
    "publish_time": "20240925",
    "authors": [
      "Hasan Alp Caferoğlu",
      "Özgür Ulusoy"
    ],
    "abstract": "Translating Natural Language Queries into Structured Query Language\n(Text-to-SQL or NLQ-to-SQL) is a critical task extensively studied by both the\nnatural language processing and database communities, aimed at providing a\nnatural language interface to databases (NLIDB) and lowering the barrier for\nnon-experts. Despite recent advancements made through the use of Large Language\nModels (LLMs), significant challenges remain. These include handling complex\ndatabase schemas, resolving ambiguity in user queries, and generating SQL\nqueries with intricate structures that accurately reflect the user's intent. In\nthis work, we introduce E-SQL, a novel pipeline specifically designed to\naddress these challenges through direct schema linking and candidate predicate\naugmentation. E-SQL enhances the natural language query by incorporating\nrelevant database items (i.e., tables, columns, and values) and conditions\ndirectly into the question and SQL construction plan, bridging the gap between\nthe query and the database structure. The pipeline leverages candidate\npredicate augmentation to mitigate erroneous or incomplete predicates in\ngenerated SQLs. Comprehensive evaluations on the BIRD benchmark illustrate that\nE-SQL achieves competitive performance, particularly excelling in complex\nqueries with a 66.29% execution accuracy on the test set. A further observation\nfrom our experiments reveals that incorporating schema filtering into the\ntranslation pipeline does not have a positive impact on performance when the\nmost advanced proprietary LLMs are used. Additionally, our experiments with\nsmall LLMs highlight the importance and positive impact of enriched questions\non their performance. Without fine-tuning, single-prompt SQL generation using\nenriched questions with DeepSeek Coder 7B Instruct 1.5v achieves 56.45%\nexecution accuracy on the BIRD development set.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2409.15228v3",
    "title": "A Comprehensive Framework for Evaluating API-oriented Code Generation in Large Language Models",
    "publish_time": "20240923",
    "authors": [
      "Yixi Wu",
      "Pengfei He",
      "Zehao Wang",
      "Shaowei Wang",
      "Yuan Tian",
      "Tse-Hsun Chen"
    ],
    "abstract": "Large language models (LLMs) like GitHub Copilot and ChatGPT have emerged as\npowerful tools for code generation, significantly enhancing productivity and\naccelerating software development. However, existing benchmarks primarily focus\non general code generation without considering API-oriented code generation,\ni.e., generating code that invokes APIs from specific libraries. Given the\ngrowing demand for API-oriented code generation, there is a pressing need for a\nsystematic and automated approach to evaluate LLM on API-oriented code\ngeneration. To address this gap, we propose AutoAPIEval, a lightweight and\nautomated framework designed to evaluate the capabilities of LLMs in\nAPI-oriented code generation. Our framework works with any library that\nprovides API documentation and focuses on two unit tasks: API recommendation\nand code example generation, along with four metrics to evaluate the generated\nAPIs and code examples, such as the proportion of incorrect API recommendations\nfor Task 1, and the proportion of code examples where no specific API is\ninvoked and uncompilable/unexecutable code examples for Task 2. In addition, we\nconducted a case study on three LLMs (ChatGPT, MagiCoder, and DeepSeek Coder)\nand Java Runtime Environment 8 to demonstrate the framework's effectiveness.\nOur findings reveal substantial variability in LLM performance across tasks,\nwith ChatGPT adhering better to instructions, while sharing similar\neffectiveness in code example generation with its counterparts (i.e., MagiCoder\nand DeekSeek Coder). We also identify key factors associated with code quality,\nsuch as API popularity and model confidence, and build classifiers that achieve\nhigh accuracy in detecting incorrect API recommendations and erroneous code\nexamples. Retrieval-augmented generation enhances the quality of code generated\nby LLMs, though its effectiveness varies across different LLMs.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2409.07131v1",
    "title": "Reranking Laws for Language Generation: A Communication-Theoretic Perspective",
    "publish_time": "20240911",
    "authors": [
      "António Farinhas",
      "Haau-Sing Li",
      "André F. T. Martins"
    ],
    "abstract": "To ensure large language models (LLMs) are used safely, one must reduce their\npropensity to hallucinate or to generate unacceptable answers. A simple and\noften used strategy is to first let the LLM generate multiple hypotheses and\nthen employ a reranker to choose the best one. In this paper, we draw a\nparallel between this strategy and the use of redundancy to decrease the error\nrate in noisy communication channels. We conceptualize the generator as a\nsender transmitting multiple descriptions of a message through parallel noisy\nchannels. The receiver decodes the message by ranking the (potentially\ncorrupted) descriptions and selecting the one found to be most reliable. We\nprovide conditions under which this protocol is asymptotically error-free\n(i.e., yields an acceptable answer almost surely) even in scenarios where the\nreranker is imperfect (governed by Mallows or Zipf-Mandelbrot models) and the\nchannel distributions are statistically dependent. We use our framework to\nobtain reranking laws which we validate empirically on two real-world tasks\nusing LLMs: text-to-code generation with DeepSeek-Coder 7B and machine\ntranslation of medical data with TowerInstruct 13B.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2409.03031v1",
    "title": "Debugging with Open-Source Large Language Models: An Evaluation",
    "publish_time": "20240904",
    "authors": [
      "Yacine Majdoub",
      "Eya Ben Charrada"
    ],
    "abstract": "Large language models have shown good potential in supporting software\ndevelopment tasks. This is why more and more developers turn to LLMs (e.g.\nChatGPT) to support them in fixing their buggy code. While this can save time\nand effort, many companies prohibit it due to strict code sharing policies. To\naddress this, companies can run open-source LLMs locally. But until now there\nis not much research evaluating the performance of open-source large language\nmodels in debugging. This work is a preliminary evaluation of the capabilities\nof open-source LLMs in fixing buggy code. The evaluation covers five\nopen-source large language models and uses the benchmark DebugBench which\nincludes more than 4000 buggy code instances written in Python, Java and C++.\nOpen-source LLMs achieved scores ranging from 43.9% to 66.6% with\nDeepSeek-Coder achieving the best score for all three programming languages.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2408.11729v2",
    "title": "LLM4VV: Exploring LLM-as-a-Judge for Validation and Verification Testsuites",
    "publish_time": "20240821",
    "authors": [
      "Zachariah Sollenberger",
      "Jay Patel",
      "Christian Munley",
      "Aaron Jarmusch",
      "Sunita Chandrasekaran"
    ],
    "abstract": "Large Language Models (LLM) are evolving and have significantly\nrevolutionized the landscape of software development. If used well, they can\nsignificantly accelerate the software development cycle. At the same time, the\ncommunity is very cautious of the models being trained on biased or sensitive\ndata, which can lead to biased outputs along with the inadvertent release of\nconfidential information. Additionally, the carbon footprints and the\nun-explainability of these black box models continue to raise questions about\nthe usability of LLMs.\n  With the abundance of opportunities LLMs have to offer, this paper explores\nthe idea of judging tests used to evaluate compiler implementations of\ndirective-based programming models as well as probe into the black box of LLMs.\nBased on our results, utilizing an agent-based prompting approach and setting\nup a validation pipeline structure drastically increased the quality of\nDeepSeek Coder, the LLM chosen for the evaluation purposes.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2408.09121v2",
    "title": "Selective Prompt Anchoring for Code Generation",
    "publish_time": "20240817",
    "authors": [
      "Yuan Tian",
      "Tianyi Zhang"
    ],
    "abstract": "Recent advances in large language models (LLMs) such as Copilot and ChatGPT\nhave transformed software development by automating coding tasks. Despite these\nadvancements, challenges remain in reducing error rates and fully meeting user\nexpectations. Our empirical study reveals LLMs tend to dilute their\nself-attention on the initial prompt as more code tokens are generated. We\nhypothesize this self-attention dilution issue is one of the root causes of\ninaccuracies in LLM-generated code. To mitigate this issue, we propose\nSelective Prompt Anchoring (SPA). SPA amplifies the influence of the selected\nparts in the initial prompt, which we refer to as ``anchored text'', during\ncode generation. Specifically, SPA calculates the logit distribution difference\nwith and without the anchored text. We prove this difference approximates the\nanchored text's contextual contribution to the output logits. SPA creates an\naugmented logit distribution by linearly combining the original logit\ndistribution and the logit difference. We evaluate SPA with five LLMs on four\nbenchmarks. Our results demonstrate that using SPA can consistently improve\nPass@1 rates by up to 9.7% in all settings. Notably, with selective text\nanchoring, a small version of DeepSeek-Coder (6.7B) can achieve better\nperformance than an original much larger version (33B). Our code is available\nat https://github.com/magic-YuanTian/Selective-Prompt-Anchoring.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2408.08152v1",
    "title": "DeepSeek-Prover-V1.5: Harnessing Proof Assistant Feedback for Reinforcement Learning and Monte-Carlo Tree Search",
    "publish_time": "20240815",
    "authors": [
      "Huajian Xin",
      "Z. Z. Ren",
      "Junxiao Song",
      "Zhihong Shao",
      "Wanjia Zhao",
      "Haocheng Wang",
      "Bo Liu",
      "Liyue Zhang",
      "Xuan Lu",
      "Qiushi Du",
      "Wenjun Gao",
      "Qihao Zhu",
      "Dejian Yang",
      "Zhibin Gou",
      "Z. F. Wu",
      "Fuli Luo",
      "Chong Ruan"
    ],
    "abstract": "We introduce DeepSeek-Prover-V1.5, an open-source language model designed for\ntheorem proving in Lean 4, which enhances DeepSeek-Prover-V1 by optimizing both\ntraining and inference processes. Pre-trained on DeepSeekMath-Base with\nspecialization in formal mathematical languages, the model undergoes supervised\nfine-tuning using an enhanced formal theorem proving dataset derived from\nDeepSeek-Prover-V1. Further refinement is achieved through reinforcement\nlearning from proof assistant feedback (RLPAF). Beyond the single-pass\nwhole-proof generation approach of DeepSeek-Prover-V1, we propose RMaxTS, a\nvariant of Monte-Carlo tree search that employs an intrinsic-reward-driven\nexploration strategy to generate diverse proof paths. DeepSeek-Prover-V1.5\ndemonstrates significant improvements over DeepSeek-Prover-V1, achieving new\nstate-of-the-art results on the test set of the high school level miniF2F\nbenchmark ($63.5\\%$) and the undergraduate level ProofNet benchmark ($25.3\\%$).",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2408.04660v3",
    "title": "XMainframe: A Large Language Model for Mainframe Modernization",
    "publish_time": "20240805",
    "authors": [
      "Anh T. V. Dau",
      "Hieu Trung Dao",
      "Anh Tuan Nguyen",
      "Hieu Trung Tran",
      "Phong X. Nguyen",
      "Nghi D. Q. Bui"
    ],
    "abstract": "Mainframe operating systems, despite their inception in the 1940s, continue\nto support critical sectors like finance and government. However, these systems\nare often viewed as outdated, requiring extensive maintenance and\nmodernization. Addressing this challenge necessitates innovative tools that can\nunderstand and interact with legacy codebases. To this end, we introduce\nXMainframe, a state-of-the-art large language model (LLM) specifically designed\nwith knowledge of mainframe legacy systems and COBOL codebases. Our solution\ninvolves the creation of an extensive data collection pipeline to produce\nhigh-quality training datasets, enhancing XMainframe's performance in this\nspecialized domain. Additionally, we present MainframeBench, a comprehensive\nbenchmark for assessing mainframe knowledge, including multiple-choice\nquestions, question answering, and COBOL code summarization. Our empirical\nevaluations demonstrate that XMainframe consistently outperforms existing\nstate-of-the-art LLMs across these tasks. Specifically, XMainframe achieves 30%\nhigher accuracy than DeepSeek-Coder on multiple-choice questions, doubles the\nBLEU score of Mixtral-Instruct 8x7B on question answering, and scores six times\nhigher than GPT-3.5 on COBOL summarization. Our work highlights the potential\nof XMainframe to drive significant advancements in managing and modernizing\nlegacy systems, thereby enhancing productivity and saving time for software\ndevelopers.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2408.02193v1",
    "title": "CodeACT: Code Adaptive Compute-efficient Tuning Framework for Code LLMs",
    "publish_time": "20240805",
    "authors": [
      "Weijie Lv",
      "Xuan Xia",
      "Sheng-Jun Huang"
    ],
    "abstract": "Large language models (LLMs) have shown great potential in code-related\ntasks, yet open-source models lag behind their closed-source counterparts. To\nbridge this performance gap, existing methods generate vast amounts of\nsynthetic data for fine-tuning, leading to inefficiencies in training.\nMotivated by the need for more effective and efficient training, we propose the\nCode Adaptive Compute-efficient Tuning (CodeACT) framework. CodeACT introduces\nthe Complexity and Diversity Aware Sampling (CDAS) method to select\nhigh-quality training data based on complexity and diversity, and the Dynamic\nPack padding strategy to reduce computational resource usage by minimizing\npadding tokens during training. Experimental results demonstrate that\nCodeACT-DeepSeek-Coder-6.7B, fine-tuned on only 40% of the EVOL-Instruct data,\nachieves an 8.6% performance increase on HumanEval, reduces training time by\n78%, and decreases peak GPU memory usage by 27%. These findings underscore\nCodeACT's ability to enhance the performance and efficiency of open-source\nmodels. By optimizing both the data selection and training processes, CodeACT\noffers a comprehensive approach to improving the capabilities of open-source\nLLMs while significantly reducing computational requirements, addressing the\ndual challenges of data quality and training efficiency, and paving the way for\nmore resource-efficient and performant models.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2408.01354v1",
    "title": "MCGMark: An Encodable and Robust Online Watermark for LLM-Generated Malicious Code",
    "publish_time": "20240802",
    "authors": [
      "Kaiwen Ning",
      "Jiachi Chen",
      "Qingyuan Zhong",
      "Tao Zhang",
      "Yanlin Wang",
      "Wei Li",
      "Yu Zhang",
      "Weizhe Zhang",
      "Zibin Zheng"
    ],
    "abstract": "With the advent of large language models (LLMs), numerous software service\nproviders (SSPs) are dedicated to developing LLMs customized for code\ngeneration tasks, such as CodeLlama and Copilot. However, these LLMs can be\nleveraged by attackers to create malicious software, which may pose potential\nthreats to the software ecosystem. For example, they can automate the creation\nof advanced phishing malware. To address this issue, we first conduct an\nempirical study and design a prompt dataset, MCGTest, which involves\napproximately 400 person-hours of work and consists of 406 malicious code\ngeneration tasks. Utilizing this dataset, we propose MCGMark, the first robust,\ncode structure-aware, and encodable watermarking approach to trace\nLLM-generated code. We embed encodable information by controlling the token\nselection and ensuring the output quality based on probabilistic outliers.\nAdditionally, we enhance the robustness of the watermark by considering the\nstructural features of malicious code, preventing the embedding of the\nwatermark in easily modified positions, such as comments. We validate the\neffectiveness and robustness of MCGMark on the DeepSeek-Coder. MCGMark achieves\nan embedding success rate of 88.9% within a maximum output limit of 400 tokens.\nFurthermore, it also demonstrates strong robustness and has minimal impact on\nthe quality of the output code. Our approach assists SSPs in tracing and\nholding responsible parties accountable for malicious code generated by LLMs.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2407.21787v3",
    "title": "Large Language Monkeys: Scaling Inference Compute with Repeated Sampling",
    "publish_time": "20240731",
    "authors": [
      "Bradley Brown",
      "Jordan Juravsky",
      "Ryan Ehrlich",
      "Ronald Clark",
      "Quoc V. Le",
      "Christopher Ré",
      "Azalia Mirhoseini"
    ],
    "abstract": "Scaling the amount of compute used to train language models has dramatically\nimproved their capabilities. However, when it comes to inference, we often\nlimit models to making only one attempt at a problem. Here, we explore\ninference compute as another axis for scaling, using the simple technique of\nrepeatedly sampling candidate solutions from a model. Across multiple tasks and\nmodels, we observe that coverage -- the fraction of problems that are solved by\nany generated sample -- scales with the number of samples over four orders of\nmagnitude. Interestingly, the relationship between coverage and the number of\nsamples is often log-linear and can be modelled with an exponentiated power\nlaw, suggesting the existence of inference-time scaling laws. In domains like\ncoding and formal proofs, where answers can be automatically verified, these\nincreases in coverage directly translate into improved performance. When we\napply repeated sampling to SWE-bench Lite, the fraction of issues solved with\nDeepSeek-Coder-V2-Instruct increases from 15.9% with one sample to 56% with 250\nsamples, outperforming the single-sample state-of-the-art of 43%. In domains\nwithout automatic verifiers, we find that common methods for picking from a\nsample collection (majority voting and reward models) plateau beyond several\nhundred samples and fail to fully scale with the sample budget.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2407.09590v3",
    "title": "Diversifying the Expert Knowledge for Task-Agnostic Pruning in Sparse Mixture-of-Experts",
    "publish_time": "20240712",
    "authors": [
      "Zeliang Zhang",
      "Xiaodong Liu",
      "Hao Cheng",
      "Chenliang Xu",
      "Jianfeng Gao"
    ],
    "abstract": "By increasing model parameters but activating them sparsely when performing a\ntask, the use of Mixture-of-Experts (MoE) architecture significantly improves\nthe performance of Large Language Models (LLMs) without increasing the\ninference cost. However, the memory consumption due to the growing number of\nexperts presents a challenge to the deployment of these models in many real\nworld settings. Our empirical study reveals that some experts encode redundant\nknowledge during pre-training. We thus propose a method of grouping and pruning\nsimilar experts to improve the model's parameter efficiency. We validate the\neffectiveness of our method by pruning three state-of-the-art MoE\narchitectures, including Mixtral, Deepseek-MoE, and Qwen. The evaluation shows\nthat our method outperforms other model pruning methods on a range of natural\nlanguage tasks. We will release our code to facilitate future research.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2407.06249v1",
    "title": "CodeUpdateArena: Benchmarking Knowledge Editing on API Updates",
    "publish_time": "20240708",
    "authors": [
      "Zeyu Leo Liu",
      "Shrey Pandit",
      "Xi Ye",
      "Eunsol Choi",
      "Greg Durrett"
    ],
    "abstract": "Large language models (LLMs) are increasingly being used to synthesize and\nreason about source code. However, the static nature of these models' knowledge\ndoes not reflect the fact that libraries and API functions they invoke are\ncontinuously evolving, with functionality being added or changing. While\nnumerous benchmarks evaluate how LLMs can generate code, no prior work has\nstudied how an LLMs' knowledge about code API functions can be updated. To fill\nthis gap, we present CodeUpdateArena, a benchmark for knowledge editing in the\ncode domain. An instance in our benchmark consists of a synthetic API function\nupdate paired with a program synthesis example that uses the updated\nfunctionality; our goal is to update an LLM to be able to solve this program\nsynthesis example without providing documentation of the update at inference\ntime. Compared to knowledge editing for facts encoded in text, success here is\nmore challenging: a code LLM must correctly reason about the semantics of the\nmodified function rather than just reproduce its syntax. Our dataset is\nconstructed by first prompting GPT-4 to generate atomic and executable function\nupdates. Then, for each update, we generate program synthesis examples whose\ncode solutions are prone to use the update. Our benchmark covers updates of\nvarious types to 54 functions from seven diverse Python packages, with a total\nof 670 program synthesis examples. Our experiments show that prepending\ndocumentation of the update to open-source code LLMs (i.e., DeepSeek,\nCodeLlama) does not allow them to incorporate changes for problem solving, and\nexisting knowledge editing techniques also have substantial room for\nimprovement. We hope our benchmark will inspire new methods for knowledge\nupdating in code LLMs.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2407.05700v2",
    "title": "InverseCoder: Self-improving Instruction-Tuned Code LLMs with Inverse-Instruct",
    "publish_time": "20240708",
    "authors": [
      "Yutong Wu",
      "Di Huang",
      "Wenxuan Shi",
      "Wei Wang",
      "Lingzhe Gao",
      "Shihao Liu",
      "Ziyuan Nan",
      "Kaizhao Yuan",
      "Rui Zhang",
      "Xishan Zhang",
      "Zidong Du",
      "Qi Guo",
      "Yewen Pu",
      "Dawei Yin",
      "Xing Hu",
      "Yunji Chen"
    ],
    "abstract": "Recent advancements in open-source code large language models (LLMs) have\nbeen driven by fine-tuning on the data generated from powerful closed-source\nLLMs, which are expensive to obtain. This paper explores whether it is possible\nto use a fine-tuned open-source model to generate additional data to augment\nits instruction-tuning dataset. We make two observations: (1) A code snippet\ncan serve as the response to different instructions. (2) Instruction-tuned code\nLLMs perform better at translating code into instructions than the reverse.\nBased on these observations, we propose Inverse-Instruct, a data augmentation\ntechnique that uses a fine-tuned LLM to generate additional instructions of\ncode responses from its own training dataset. The additional\ninstruction-response pairs are added to the original dataset, and a stronger\ncode LLM can be obtained by fine-tuning on the augmented dataset. We\nempirically validate Inverse-Instruct on a range of open-source code models\n(e.g. CodeLlama-Python and DeepSeek-Coder) and benchmarks (e.g., HumanEval(+),\nMBPP(+), DS-1000 and MultiPL-E), showing it consistently improves the base\nmodels.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2407.04078v3",
    "title": "DotaMath: Decomposition of Thought with Code Assistance and Self-correction for Mathematical Reasoning",
    "publish_time": "20240704",
    "authors": [
      "Chengpeng Li",
      "Guanting Dong",
      "Mingfeng Xue",
      "Ru Peng",
      "Xiang Wang",
      "Dayiheng Liu"
    ],
    "abstract": "Large language models (LLMs) have made impressive progress in handling simple\nmath problems, yet they still struggle with more challenging and complex\nmathematical tasks. In this paper, we introduce a series of LLMs that employs\nthe Decomposition of thought with code assistance and self-correction for\nmathematical reasoning, dubbed as DotaMath. DotaMath models tackle complex\nmathematical tasks by decomposing them into simpler logical subtasks,\nleveraging code to solve these subtasks, obtaining fine-grained feedback from\nthe code interpreter, and engaging in self-reflection and correction. By\nannotating diverse interactive tool-use trajectories and employing query\nevolution on GSM8K and MATH datasets, we generate an instruction fine-tuning\ndataset called DotaMathQA with 574K query-response pairs. We train a series of\nbase LLMs using imitation learning on DotaMathQA, resulting in DotaMath models\nthat achieve remarkable performance compared to open-source LLMs across various\nin-domain and out-of-domain benchmarks. Notably, DotaMath-deepseek-7B showcases\nan outstanding performance of 64.8% on the competitive MATH dataset and 86.7%\non GSM8K. Besides, DotaMath-deepseek-7B maintains strong competitiveness on a\nseries of in-domain and out-of-domain benchmarks (Avg. 80.1%). Looking forward,\nwe anticipate that the DotaMath paradigm will open new pathways for addressing\nintricate mathematical problems. Our code is publicly available at\nhttps://github.com/ChengpengLi1003/DotaMath.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2407.03157v1",
    "title": "Let the Code LLM Edit Itself When You Edit the Code",
    "publish_time": "20240703",
    "authors": [
      "Zhenyu He",
      "Jun Zhang",
      "Shengjie Luo",
      "Jingjing Xu",
      "Zhi Zhang",
      "Di He"
    ],
    "abstract": "In this work, we investigate a typical scenario in code generation where a\ndeveloper edits existing code in real time and requests a code assistant, e.g.,\na large language model, to re-predict the next token or next line on the fly.\nNaively, the LLM needs to re-encode the entire KV cache to provide an accurate\nprediction. However, this process is computationally expensive, especially when\nthe sequence length is long. Simply encoding the edited subsequence and\nintegrating it to the original KV cache meets the temporal confusion problem,\nleading to significantly worse performance. We address this efficiency and\naccuracy trade-off by introducing \\underline{\\textbf{Positional\n\\textbf{I}ntegrity \\textbf{E}ncoding} (PIE). Building upon the rotary\npositional encoding, PIE first removes the rotary matrices in the Key cache\nthat introduce temporal confusion and then reapplies the correct rotary\nmatrices. This process ensures that positional relationships between tokens are\ncorrect and requires only a single round of matrix multiplication. We validate\nthe effectiveness of PIE through extensive experiments on the RepoBench-C-8k\ndataset, utilizing DeepSeek-Coder models with 1.3B, 6.7B, and 33B parameters.\nOur evaluation includes three real-world coding tasks: code insertion, code\ndeletion, and multi-place code editing. Results demonstrate that PIE reduces\ncomputational overhead by over 85% compared to the standard full recomputation\napproach across all model sizes and tasks while well approximating the model\nperformance.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2407.01906v2",
    "title": "Let the Expert Stick to His Last: Expert-Specialized Fine-Tuning for Sparse Architectural Large Language Models",
    "publish_time": "20240702",
    "authors": [
      "Zihan Wang",
      "Deli Chen",
      "Damai Dai",
      "Runxin Xu",
      "Zhuoshu Li",
      "Y. Wu"
    ],
    "abstract": "Parameter-efficient fine-tuning (PEFT) is crucial for customizing Large\nLanguage Models (LLMs) with constrained resources. Although there have been\nvarious PEFT methods for dense-architecture LLMs, PEFT for sparse-architecture\nLLMs is still underexplored. In this work, we study the PEFT method for LLMs\nwith the Mixture-of-Experts (MoE) architecture and the contents of this work\nare mainly threefold: (1) We investigate the dispersion degree of the activated\nexperts in customized tasks, and found that the routing distribution for a\nspecific task tends to be highly concentrated, while the distribution of\nactivated experts varies significantly across different tasks. (2) We propose\nExpert-Specialized Fine-Tuning, or ESFT, which tunes the experts most relevant\nto downstream tasks while freezing the other experts and modules; experimental\nresults demonstrate that our method not only improves the tuning efficiency,\nbut also matches or even surpasses the performance of full-parameter\nfine-tuning. (3) We further analyze the impact of the MoE architecture on\nexpert-specialized fine-tuning. We find that MoE models with finer-grained\nexperts are more advantageous in selecting the combination of experts that are\nmost relevant to downstream tasks, thereby enhancing both the training\nefficiency and effectiveness. Our code is available at\nhttps://github.com/deepseek-ai/ESFT.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2406.11931v1",
    "title": "DeepSeek-Coder-V2: Breaking the Barrier of Closed-Source Models in Code Intelligence",
    "publish_time": "20240617",
    "authors": [
      "DeepSeek-AI",
      "Qihao Zhu",
      "Daya Guo",
      "Zhihong Shao",
      "Dejian Yang",
      "Peiyi Wang",
      "Runxin Xu",
      "Y. Wu",
      "Yukun Li",
      "Huazuo Gao",
      "Shirong Ma",
      "Wangding Zeng",
      "Xiao Bi",
      "Zihui Gu",
      "Hanwei Xu",
      "Damai Dai",
      "Kai Dong",
      "Liyue Zhang",
      "Yishi Piao",
      "Zhibin Gou",
      "Zhenda Xie",
      "Zhewen Hao",
      "Bingxuan Wang",
      "Junxiao Song",
      "Deli Chen",
      "Xin Xie",
      "Kang Guan",
      "Yuxiang You",
      "Aixin Liu",
      "Qiushi Du",
      "Wenjun Gao",
      "Xuan Lu",
      "Qinyu Chen",
      "Yaohui Wang",
      "Chengqi Deng",
      "Jiashi Li",
      "Chenggang Zhao",
      "Chong Ruan",
      "Fuli Luo",
      "Wenfeng Liang"
    ],
    "abstract": "We present DeepSeek-Coder-V2, an open-source Mixture-of-Experts (MoE) code\nlanguage model that achieves performance comparable to GPT4-Turbo in\ncode-specific tasks. Specifically, DeepSeek-Coder-V2 is further pre-trained\nfrom an intermediate checkpoint of DeepSeek-V2 with additional 6 trillion\ntokens. Through this continued pre-training, DeepSeek-Coder-V2 substantially\nenhances the coding and mathematical reasoning capabilities of DeepSeek-V2,\nwhile maintaining comparable performance in general language tasks. Compared to\nDeepSeek-Coder-33B, DeepSeek-Coder-V2 demonstrates significant advancements in\nvarious aspects of code-related tasks, as well as reasoning and general\ncapabilities. Additionally, DeepSeek-Coder-V2 expands its support for\nprogramming languages from 86 to 338, while extending the context length from\n16K to 128K. In standard benchmark evaluations, DeepSeek-Coder-V2 achieves\nsuperior performance compared to closed-source models such as GPT4-Turbo,\nClaude 3 Opus, and Gemini 1.5 Pro in coding and math benchmarks.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2406.11402v2",
    "title": "Are Small Language Models Ready to Compete with Large Language Models for Practical Applications?",
    "publish_time": "20240617",
    "authors": [
      "Neelabh Sinha",
      "Vinija Jain",
      "Aman Chadha"
    ],
    "abstract": "The rapid rise of Language Models (LMs) has expanded their use in several\napplications. Yet, due to constraints of model size, associated cost, or\nproprietary restrictions, utilizing state-of-the-art (SOTA) LLMs is not always\nfeasible. With open, smaller LMs emerging, more applications can leverage their\ncapabilities, but selecting the right LM can be challenging as smaller LMs\ndon't perform well universally. This work tries to bridge this gap by proposing\na framework to experimentally evaluate small, open LMs in practical settings\nthrough measuring semantic correctness of outputs across three practical\naspects: task types, application domains and reasoning types, using diverse\nprompt styles. It also conducts an in-depth comparison of 10 small, open LMs to\nidentify best LM and prompt style depending on specific application requirement\nusing the proposed framework. We also show that if selected appropriately, they\ncan outperform SOTA LLMs like DeepSeek-v2, GPT-4o-mini, Gemini-1.5-Pro, and\neven compete with GPT-4o.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2406.10591v1",
    "title": "MINT: a Multi-modal Image and Narrative Text Dubbing Dataset for Foley Audio Content Planning and Generation",
    "publish_time": "20240615",
    "authors": [
      "Ruibo Fu",
      "Shuchen Shi",
      "Hongming Guo",
      "Tao Wang",
      "Chunyu Qiang",
      "Zhengqi Wen",
      "Jianhua Tao",
      "Xin Qi",
      "Yi Lu",
      "Xiaopeng Wang",
      "Zhiyong Wang",
      "Yukun Liu",
      "Xuefei Liu",
      "Shuai Zhang",
      "Guanjun Li"
    ],
    "abstract": "Foley audio, critical for enhancing the immersive experience in multimedia\ncontent, faces significant challenges in the AI-generated content (AIGC)\nlandscape. Despite advancements in AIGC technologies for text and image\ngeneration, the foley audio dubbing remains rudimentary due to difficulties in\ncross-modal scene matching and content correlation. Current text-to-audio\ntechnology, which relies on detailed and acoustically relevant textual\ndescriptions, falls short in practical video dubbing applications. Existing\ndatasets like AudioSet, AudioCaps, Clotho, Sound-of-Story, and WavCaps do not\nfully meet the requirements for real-world foley audio dubbing task. To address\nthis, we introduce the Multi-modal Image and Narrative Text Dubbing Dataset\n(MINT), designed to enhance mainstream dubbing tasks such as literary story\naudiobooks dubbing, image/silent video dubbing. Besides, to address the\nlimitations of existing TTA technology in understanding and planning complex\nprompts, a Foley Audio Content Planning, Generation, and Alignment (CPGA)\nframework is proposed, which includes a content planning module leveraging\nlarge language models for complex multi-modal prompts comprehension.\nAdditionally, the training process is optimized using Proximal Policy\nOptimization based reinforcement learning, significantly improving the\nalignment and auditory realism of generated foley audio. Experimental results\ndemonstrate that our approach significantly advances the field of foley audio\ndubbing, providing robust solutions for the challenges of multi-modal dubbing.\nEven when utilizing the relatively lightweight GPT-2 model, our framework\noutperforms open-source multimodal large models such as LLaVA, DeepSeek-VL, and\nMoondream2. The dataset is available at https://github.com/borisfrb/MINT .",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2405.19856v1",
    "title": "DevEval: A Manually-Annotated Code Generation Benchmark Aligned with Real-World Code Repositories",
    "publish_time": "20240530",
    "authors": [
      "Jia Li",
      "Ge Li",
      "Yunfei Zhao",
      "Yongmin Li",
      "Huanyu Liu",
      "Hao Zhu",
      "Lecheng Wang",
      "Kaibo Liu",
      "Zheng Fang",
      "Lanshen Wang",
      "Jiazheng Ding",
      "Xuanming Zhang",
      "Yuqi Zhu",
      "Yihong Dong",
      "Zhi Jin",
      "Binhua Li",
      "Fei Huang",
      "Yongbin Li"
    ],
    "abstract": "How to evaluate the coding abilities of Large Language Models (LLMs) remains\nan open question. We find that existing benchmarks are poorly aligned with\nreal-world code repositories and are insufficient to evaluate the coding\nabilities of LLMs.\n  To address the knowledge gap, we propose a new benchmark named DevEval, which\nhas three advances. (1) DevEval aligns with real-world repositories in multiple\ndimensions, e.g., code distributions and dependency distributions. (2) DevEval\nis annotated by 13 developers and contains comprehensive annotations (e.g.,\nrequirements, original repositories, reference code, and reference\ndependencies). (3) DevEval comprises 1,874 testing samples from 117\nrepositories, covering 10 popular domains (e.g., Internet, Database). Based on\nDevEval, we propose repository-level code generation and evaluate 8 popular\nLLMs on DevEval (e.g., gpt-4, gpt-3.5, StarCoder 2, DeepSeek Coder, CodeLLaMa).\nOur experiments reveal these LLMs' coding abilities in real-world code\nrepositories. For example, in our experiments, the highest Pass@1 of\ngpt-4-turbo is only 53.04%. We also analyze LLMs' failed cases and summarize\ntheir shortcomings. We hope DevEval can facilitate the development of LLMs in\nreal code repositories. DevEval, prompts, and LLMs' predictions have been\nreleased.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2405.19250v1",
    "title": "Kotlin ML Pack: Technical Report",
    "publish_time": "20240529",
    "authors": [
      "Sergey Titov",
      "Mikhail Evtikhiev",
      "Anton Shapkin",
      "Oleg Smirnov",
      "Sergei Boytsov",
      "Sergei Boytsov",
      "Dariia Karaeva",
      "Maksim Sheptyakov",
      "Mikhail Arkhipov",
      "Timofey Bryksin",
      "Egor Bogomolov"
    ],
    "abstract": "In this technical report, we present three novel datasets of Kotlin code:\nKStack, KStack-clean, and KExercises. We also describe the results of\nfine-tuning CodeLlama and DeepSeek models on this data. Additionally, we\npresent a version of the HumanEval benchmark rewritten by human experts into\nKotlin - both the solutions and the tests. Our results demonstrate that small,\nhigh-quality datasets (KStack-clean and KExercises) can significantly improve\nmodel performance on code generation tasks, achieving up to a 16-point increase\nin pass rate on the HumanEval benchmark. Lastly, we discuss potential future\nwork in the field of improving language modeling for Kotlin, including the use\nof static analysis tools in the learning process and the introduction of more\nintricate and realistic benchmarks.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2405.17057v1",
    "title": "ReflectionCoder: Learning from Reflection Sequence for Enhanced One-off Code Generation",
    "publish_time": "20240527",
    "authors": [
      "Houxing Ren",
      "Mingjie Zhan",
      "Zhongyuan Wu",
      "Aojun Zhou",
      "Junting Pan",
      "Hongsheng Li"
    ],
    "abstract": "Code generation plays a crucial role in various tasks, such as code\nauto-completion and mathematical reasoning. Previous work has proposed numerous\nmethods to enhance code generation performance, including integrating feedback\nfrom the compiler. Inspired by this, we present ReflectionCoder, a novel\napproach that effectively leverages reflection sequences constructed by\nintegrating compiler feedback to improve one-off code generation performance.\nFurthermore, we propose reflection self-distillation and dynamically masked\ndistillation to effectively utilize these reflection sequences. Extensive\nexperiments on three benchmarks, i.e., HumanEval (+), MBPP (+), and MultiPl-E,\ndemonstrate that models fine-tuned with our method achieve state-of-the-art\nperformance. Notably, ReflectionCoder-DeepSeek-Coder-33B reaches pass@1 of 82.9\n(76.8) on HumanEval (+) and 84.1 (72.0) on MBPP (+), on par with GPT-3.5-Turbo\nand Claude-3-opus, and surpasses early GPT-4. Beyond the code domain, we\nbelieve this approach can benefit other domains that focus on final results and\nrequire long reasoning paths. Code and data are available at\nhttps://github.com/SenseLLM/ReflectionCoder.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2405.04434v5",
    "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
    "publish_time": "20240507",
    "authors": [
      "DeepSeek-AI",
      "Aixin Liu",
      "Bei Feng",
      "Bin Wang",
      "Bingxuan Wang",
      "Bo Liu",
      "Chenggang Zhao",
      "Chengqi Dengr",
      "Chong Ruan",
      "Damai Dai",
      "Daya Guo",
      "Dejian Yang",
      "Deli Chen",
      "Dongjie Ji",
      "Erhang Li",
      "Fangyun Lin",
      "Fuli Luo",
      "Guangbo Hao",
      "Guanting Chen",
      "Guowei Li",
      "H. Zhang",
      "Hanwei Xu",
      "Hao Yang",
      "Haowei Zhang",
      "Honghui Ding",
      "Huajian Xin",
      "Huazuo Gao",
      "Hui Li",
      "Hui Qu",
      "J. L. Cai",
      "Jian Liang",
      "Jianzhong Guo",
      "Jiaqi Ni",
      "Jiashi Li",
      "Jin Chen",
      "Jingyang Yuan",
      "Junjie Qiu",
      "Junxiao Song",
      "Kai Dong",
      "Kaige Gao",
      "Kang Guan",
      "Lean Wang",
      "Lecong Zhang",
      "Lei Xu",
      "Leyi Xia",
      "Liang Zhao",
      "Liyue Zhang",
      "Meng Li",
      "Miaojun Wang",
      "Mingchuan Zhang",
      "Minghua Zhang",
      "Minghui Tang",
      "Mingming Li",
      "Ning Tian",
      "Panpan Huang",
      "Peiyi Wang",
      "Peng Zhang",
      "Qihao Zhu",
      "Qinyu Chen",
      "Qiushi Du",
      "R. J. Chen",
      "R. L. Jin",
      "Ruiqi Ge",
      "Ruizhe Pan",
      "Runxin Xu",
      "Ruyi Chen",
      "S. S. Li",
      "Shanghao Lu",
      "Shangyan Zhou",
      "Shanhuang Chen",
      "Shaoqing Wu",
      "Shengfeng Ye",
      "Shirong Ma",
      "Shiyu Wang",
      "Shuang Zhou",
      "Shuiping Yu",
      "Shunfeng Zhou",
      "Size Zheng",
      "T. Wang",
      "Tian Pei",
      "Tian Yuan",
      "Tianyu Sun",
      "W. L. Xiao",
      "Wangding Zeng",
      "Wei An",
      "Wen Liu",
      "Wenfeng Liang",
      "Wenjun Gao",
      "Wentao Zhang",
      "X. Q. Li",
      "Xiangyue Jin",
      "Xianzu Wang",
      "Xiao Bi",
      "Xiaodong Liu",
      "Xiaohan Wang",
      "Xiaojin Shen",
      "Xiaokang Chen",
      "Xiaosha Chen",
      "Xiaotao Nie",
      "Xiaowen Sun",
      "Xiaoxiang Wang",
      "Xin Liu",
      "Xin Xie",
      "Xingkai Yu",
      "Xinnan Song",
      "Xinyi Zhou",
      "Xinyu Yang",
      "Xuan Lu",
      "Xuecheng Su",
      "Y. Wu",
      "Y. K. Li",
      "Y. X. Wei",
      "Y. X. Zhu",
      "Yanhong Xu",
      "Yanping Huang",
      "Yao Li",
      "Yao Zhao",
      "Yaofeng Sun",
      "Yaohui Li",
      "Yaohui Wang",
      "Yi Zheng",
      "Yichao Zhang",
      "Yiliang Xiong",
      "Yilong Zhao",
      "Ying He",
      "Ying Tang",
      "Yishi Piao",
      "Yixin Dong",
      "Yixuan Tan",
      "Yiyuan Liu",
      "Yongji Wang",
      "Yongqiang Guo",
      "Yuchen Zhu",
      "Yuduan Wang",
      "Yuheng Zou",
      "Yukun Zha",
      "Yunxian Ma",
      "Yuting Yan",
      "Yuxiang You",
      "Yuxuan Liu",
      "Z. Z. Ren",
      "Zehui Ren",
      "Zhangli Sha",
      "Zhe Fu",
      "Zhen Huang",
      "Zhen Zhang",
      "Zhenda Xie",
      "Zhewen Hao",
      "Zhihong Shao",
      "Zhiniu Wen",
      "Zhipeng Xu",
      "Zhongyu Zhang",
      "Zhuoshu Li",
      "Zihan Wang",
      "Zihui Gu",
      "Zilin Li",
      "Ziwei Xie"
    ],
    "abstract": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model\ncharacterized by economical training and efficient inference. It comprises 236B\ntotal parameters, of which 21B are activated for each token, and supports a\ncontext length of 128K tokens. DeepSeek-V2 adopts innovative architectures\nincluding Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees\nefficient inference through significantly compressing the Key-Value (KV) cache\ninto a latent vector, while DeepSeekMoE enables training strong models at an\neconomical cost through sparse computation. Compared with DeepSeek 67B,\nDeepSeek-V2 achieves significantly stronger performance, and meanwhile saves\n42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum\ngeneration throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality\nand multi-source corpus consisting of 8.1T tokens, and further perform\nSupervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock\nits potential. Evaluation results show that, even with only 21B activated\nparameters, DeepSeek-V2 and its chat versions still achieve top-tier\nperformance among open-source models.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2404.18567v1",
    "title": "Assessing Cybersecurity Vulnerabilities in Code Large Language Models",
    "publish_time": "20240429",
    "authors": [
      "Md Imran Hossen",
      "Jianyi Zhang",
      "Yinzhi Cao",
      "Xiali Hei"
    ],
    "abstract": "Instruction-tuned Code Large Language Models (Code LLMs) are increasingly\nutilized as AI coding assistants and integrated into various applications.\nHowever, the cybersecurity vulnerabilities and implications arising from the\nwidespread integration of these models are not yet fully understood due to\nlimited research in this domain. To bridge this gap, this paper presents\nEvilInstructCoder, a framework specifically designed to assess the\ncybersecurity vulnerabilities of instruction-tuned Code LLMs to adversarial\nattacks. EvilInstructCoder introduces the Adversarial Code Injection Engine to\nautomatically generate malicious code snippets and inject them into benign code\nto poison instruction tuning datasets. It incorporates practical threat models\nto reflect real-world adversaries with varying capabilities and evaluates the\nexploitability of instruction-tuned Code LLMs under these diverse adversarial\nattack scenarios. Through the use of EvilInstructCoder, we conduct a\ncomprehensive investigation into the exploitability of instruction tuning for\ncoding tasks using three state-of-the-art Code LLM models: CodeLlama,\nDeepSeek-Coder, and StarCoder2, under various adversarial attack scenarios. Our\nexperimental results reveal a significant vulnerability in these models,\ndemonstrating that adversaries can manipulate the models to generate malicious\npayloads within benign code contexts in response to natural language\ninstructions. For instance, under the backdoor attack setting, by poisoning\nonly 81 samples (0.5\\% of the entire instruction dataset), we achieve Attack\nSuccess Rate at 1 (ASR@1) scores ranging from 76\\% to 86\\% for different model\nfamilies. Our study sheds light on the critical cybersecurity vulnerabilities\nposed by instruction-tuned Code LLMs and emphasizes the urgent necessity for\nrobust defense mechanisms to mitigate the identified vulnerabilities.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2404.16645v1",
    "title": "Tele-FLM Technical Report",
    "publish_time": "20240425",
    "authors": [
      "Xiang Li",
      "Yiqun Yao",
      "Xin Jiang",
      "Xuezhi Fang",
      "Chao Wang",
      "Xinzhang Liu",
      "Zihan Wang",
      "Yu Zhao",
      "Xin Wang",
      "Yuyao Huang",
      "Shuangyong Song",
      "Yongxiang Li",
      "Zheng Zhang",
      "Bo Zhao",
      "Aixin Sun",
      "Yequan Wang",
      "Zhongjiang He",
      "Zhongyuan Wang",
      "Xuelong Li",
      "Tiejun Huang"
    ],
    "abstract": "Large language models (LLMs) have showcased profound capabilities in language\nunderstanding and generation, facilitating a wide array of applications.\nHowever, there is a notable paucity of detailed, open-sourced methodologies on\nefficiently scaling LLMs beyond 50 billion parameters with minimum\ntrial-and-error cost and computational resources. In this report, we introduce\nTele-FLM (aka FLM-2), a 52B open-sourced multilingual large language model that\nfeatures a stable, efficient pre-training paradigm and enhanced factual\njudgment capabilities. Tele-FLM demonstrates superior multilingual language\nmodeling abilities, measured by BPB on textual corpus. Besides, in both English\nand Chinese foundation model evaluation, it is comparable to strong\nopen-sourced models that involve larger pre-training FLOPs, such as Llama2-70B\nand DeepSeek-67B. In addition to the model weights, we share the core designs,\nengineering practices, and training details, which we expect to benefit both\nthe academic and industrial communities.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2404.02852v1",
    "title": "Toward Inference-optimal Mixture-of-Expert Large Language Models",
    "publish_time": "20240403",
    "authors": [
      "Longfei Yun",
      "Yonghao Zhuang",
      "Yao Fu",
      "Eric P Xing",
      "Hao Zhang"
    ],
    "abstract": "Mixture-of-Expert (MoE) based large language models (LLMs), such as the\nrecent Mixtral and DeepSeek-MoE, have shown great promise in scaling model size\nwithout suffering from the quadratic growth of training cost of dense\ntransformers. Like dense models, training MoEs requires answering the same\nquestion: given a training budget, what is the optimal allocation on the model\nsize and number of tokens? We study the scaling law of MoE-based LLMs regarding\nthe relations between the model performance, model size, dataset size, and the\nexpert degree. Echoing previous research studying MoE in different contexts, we\nobserve the diminishing return of increasing the number of experts, but this\nseems to suggest we should scale the number of experts until saturation, as the\ntraining cost would remain constant, which is problematic during inference\ntime. We propose to amend the scaling law of MoE by introducing inference\nefficiency as another metric besides the validation loss. We find that MoEs\nwith a few (4/8) experts are the most serving efficient solution under the same\nperformance, but costs 2.5-3.5x more in training. On the other hand, training a\n(16/32) expert MoE much smaller (70-85%) than the loss-optimal solution, but\nwith a larger training dataset is a promising setup under a training budget.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2404.00599v1",
    "title": "EvoCodeBench: An Evolving Code Generation Benchmark Aligned with Real-World Code Repositories",
    "publish_time": "20240331",
    "authors": [
      "Jia Li",
      "Ge Li",
      "Xuanming Zhang",
      "Yihong Dong",
      "Zhi Jin"
    ],
    "abstract": "How to evaluate Large Language Models (LLMs) in code generation is an open\nquestion. Existing benchmarks demonstrate poor alignment with real-world code\nrepositories and are insufficient to evaluate the coding abilities of LLMs.\nThis paper proposes a new benchmark - EvoCodeBench to address the preceding\nproblems, which has three primary advances. (1) EvoCodeBench aligns with\nreal-world repositories in multiple dimensions, e.g., code distributions and\ndependency distributions. (2) EvoCodeBench offers comprehensive annotations\n(e.g., requirements, reference code, and reference dependencies), and robust\nevaluation metrics (e.g., Pass@k and Recall@k). (3) EvoCodeBench is an evolving\nbenchmark to avoid data leakage. We build an automatic pipeline to update\nEvoCodeBench from the latest repositories. We release the first version -\nEvoCodeBench-2403, containing 275 samples from 25 real-world repositories.\nBased on EvoCodeBench, we propose repository-level code generation and evaluate\n10 popular LLMs (e.g., gpt-4, gpt-3.5, DeepSeek Coder, StarCoder 2, CodeLLaMa,\nGemma, and Qwen 1.5). Our experiments reveal the coding abilities of these LLMs\nin real-world repositories. For example, the highest Pass@1 of gpt-4 only is\n20.73% in our experiments. We also analyze failed cases and summarize the\nshortcomings of existing LLMs in EvoCodeBench. We release EvoCodeBench, all\nprompts, and LLMs' completions for further community analysis.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2403.05525v2",
    "title": "DeepSeek-VL: Towards Real-World Vision-Language Understanding",
    "publish_time": "20240308",
    "authors": [
      "Haoyu Lu",
      "Wen Liu",
      "Bo Zhang",
      "Bingxuan Wang",
      "Kai Dong",
      "Bo Liu",
      "Jingxiang Sun",
      "Tongzheng Ren",
      "Zhuoshu Li",
      "Hao Yang",
      "Yaofeng Sun",
      "Chengqi Deng",
      "Hanwei Xu",
      "Zhenda Xie",
      "Chong Ruan"
    ],
    "abstract": "We present DeepSeek-VL, an open-source Vision-Language (VL) Model designed\nfor real-world vision and language understanding applications. Our approach is\nstructured around three key dimensions:\n  We strive to ensure our data is diverse, scalable, and extensively covers\nreal-world scenarios including web screenshots, PDFs, OCR, charts, and\nknowledge-based content, aiming for a comprehensive representation of practical\ncontexts. Further, we create a use case taxonomy from real user scenarios and\nconstruct an instruction tuning dataset accordingly. The fine-tuning with this\ndataset substantially improves the model's user experience in practical\napplications. Considering efficiency and the demands of most real-world\nscenarios, DeepSeek-VL incorporates a hybrid vision encoder that efficiently\nprocesses high-resolution images (1024 x 1024), while maintaining a relatively\nlow computational overhead. This design choice ensures the model's ability to\ncapture critical semantic and detailed information across various visual tasks.\nWe posit that a proficient Vision-Language Model should, foremost, possess\nstrong language abilities. To ensure the preservation of LLM capabilities\nduring pretraining, we investigate an effective VL pretraining strategy by\nintegrating LLM training from the beginning and carefully managing the\ncompetitive dynamics observed between vision and language modalities.\n  The DeepSeek-VL family (both 1.3B and 7B models) showcases superior user\nexperiences as a vision-language chatbot in real-world applications, achieving\nstate-of-the-art or competitive performance across a wide range of\nvisual-language benchmarks at the same model size while maintaining robust\nperformance on language-centric benchmarks. We have made both 1.3B and 7B\nmodels publicly accessible to foster innovations based on this foundation\nmodel.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2402.17644v2",
    "title": "Are LLMs Capable of Data-based Statistical and Causal Reasoning? Benchmarking Advanced Quantitative Reasoning with Data",
    "publish_time": "20240227",
    "authors": [
      "Xiao Liu",
      "Zirui Wu",
      "Xueqing Wu",
      "Pan Lu",
      "Kai-Wei Chang",
      "Yansong Feng"
    ],
    "abstract": "Quantitative reasoning is a critical skill to analyze data, yet the\nassessment of such ability remains limited. To address this gap, we introduce\nthe Quantitative Reasoning with Data (QRData) benchmark, aiming to evaluate\nLarge Language Models' capability in statistical and causal reasoning with\nreal-world data. The benchmark comprises a carefully constructed dataset of 411\nquestions accompanied by data sheets from textbooks, online learning materials,\nand academic papers. To compare models' quantitative reasoning abilities on\ndata and text, we enrich the benchmark with an auxiliary set of 290 text-only\nquestions, namely QRText. We evaluate natural language reasoning, program-based\nreasoning, and agent reasoning methods including Chain-of-Thought,\nProgram-of-Thoughts, ReAct, and code interpreter assistants on diverse models.\nThe strongest model GPT-4 achieves an accuracy of 58%, which has much room for\nimprovement. Among open-source models, Deepseek-coder-instruct, a code LLM\npretrained on 2T tokens, gets the highest accuracy of 37%. Analysis reveals\nthat models encounter difficulties in data analysis and causal reasoning, and\nstruggle in using causal knowledge and provided data simultaneously. Code and\ndata are in https://github.com/xxxiaol/QRData.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2402.11436v2",
    "title": "Pride and Prejudice: LLM Amplifies Self-Bias in Self-Refinement",
    "publish_time": "20240218",
    "authors": [
      "Wenda Xu",
      "Guanglei Zhu",
      "Xuandong Zhao",
      "Liangming Pan",
      "Lei Li",
      "William Yang Wang"
    ],
    "abstract": "Recent studies show that large language models (LLMs) improve their\nperformance through self-feedback on certain tasks while degrade on others. We\ndiscovered that such a contrary is due to LLM's bias in evaluating their own\noutput. In this paper, we formally define LLM's self-bias - the tendency to\nfavor its own generation - using two statistics. We analyze six LLMs (GPT-4,\nGPT-3.5, Gemini, LLaMA2, Mixtral and DeepSeek) on translation, constrained text\ngeneration, and mathematical reasoning tasks. We find that self-bias is\nprevalent in all examined LLMs across multiple languages and tasks. Our\nanalysis reveals that while the self-refine pipeline improves the fluency and\nunderstandability of model outputs, it further amplifies self-bias. To mitigate\nsuch biases, we discover that larger model size and external feedback with\naccurate assessment can significantly reduce bias in the self-refine pipeline,\nleading to actual performance improvement in downstream tasks. The code and\ndata are released at https://github.com/xu1998hz/llm_self_bias.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2402.03300v3",
    "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
    "publish_time": "20240205",
    "authors": [
      "Zhihong Shao",
      "Peiyi Wang",
      "Qihao Zhu",
      "Runxin Xu",
      "Junxiao Song",
      "Xiao Bi",
      "Haowei Zhang",
      "Mingchuan Zhang",
      "Y. K. Li",
      "Y. Wu",
      "Daya Guo"
    ],
    "abstract": "Mathematical reasoning poses a significant challenge for language models due\nto its complex and structured nature. In this paper, we introduce DeepSeekMath\n7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B\nmath-related tokens sourced from Common Crawl, together with natural language\nand code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the\ncompetition-level MATH benchmark without relying on external toolkits and\nvoting techniques, approaching the performance level of Gemini-Ultra and GPT-4.\nSelf-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH.\nThe mathematical reasoning capability of DeepSeekMath is attributed to two key\nfactors: First, we harness the significant potential of publicly available web\ndata through a meticulously engineered data selection pipeline. Second, we\nintroduce Group Relative Policy Optimization (GRPO), a variant of Proximal\nPolicy Optimization (PPO), that enhances mathematical reasoning abilities while\nconcurrently optimizing the memory usage of PPO.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2401.14196v2",
    "title": "DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence",
    "publish_time": "20240125",
    "authors": [
      "Daya Guo",
      "Qihao Zhu",
      "Dejian Yang",
      "Zhenda Xie",
      "Kai Dong",
      "Wentao Zhang",
      "Guanting Chen",
      "Xiao Bi",
      "Y. Wu",
      "Y. K. Li",
      "Fuli Luo",
      "Yingfei Xiong",
      "Wenfeng Liang"
    ],
    "abstract": "The rapid development of large language models has revolutionized code\nintelligence in software development. However, the predominance of\nclosed-source models has restricted extensive research and development. To\naddress this, we introduce the DeepSeek-Coder series, a range of open-source\ncode models with sizes from 1.3B to 33B, trained from scratch on 2 trillion\ntokens. These models are pre-trained on a high-quality project-level code\ncorpus and employ a fill-in-the-blank task with a 16K window to enhance code\ngeneration and infilling. Our extensive evaluations demonstrate that\nDeepSeek-Coder not only achieves state-of-the-art performance among open-source\ncode models across multiple benchmarks but also surpasses existing\nclosed-source models like Codex and GPT-3.5. Furthermore, DeepSeek-Coder models\nare under a permissive license that allows for both research and unrestricted\ncommercial use.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2401.06066v1",
    "title": "DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models",
    "publish_time": "20240111",
    "authors": [
      "Damai Dai",
      "Chengqi Deng",
      "Chenggang Zhao",
      "R. X. Xu",
      "Huazuo Gao",
      "Deli Chen",
      "Jiashi Li",
      "Wangding Zeng",
      "Xingkai Yu",
      "Y. Wu",
      "Zhenda Xie",
      "Y. K. Li",
      "Panpan Huang",
      "Fuli Luo",
      "Chong Ruan",
      "Zhifang Sui",
      "Wenfeng Liang"
    ],
    "abstract": "In the era of large language models, Mixture-of-Experts (MoE) is a promising\narchitecture for managing computational costs when scaling up model parameters.\nHowever, conventional MoE architectures like GShard, which activate the top-$K$\nout of $N$ experts, face challenges in ensuring expert specialization, i.e.\neach expert acquires non-overlapping and focused knowledge. In response, we\npropose the DeepSeekMoE architecture towards ultimate expert specialization. It\ninvolves two principal strategies: (1) finely segmenting the experts into $mN$\nones and activating $mK$ from them, allowing for a more flexible combination of\nactivated experts; (2) isolating $K_s$ experts as shared ones, aiming at\ncapturing common knowledge and mitigating redundancy in routed experts.\nStarting from a modest scale with 2B parameters, we demonstrate that\nDeepSeekMoE 2B achieves comparable performance with GShard 2.9B, which has 1.5\ntimes the expert parameters and computation. In addition, DeepSeekMoE 2B nearly\napproaches the performance of its dense counterpart with the same number of\ntotal parameters, which set the upper bound of MoE models. Subsequently, we\nscale up DeepSeekMoE to 16B parameters and show that it achieves comparable\nperformance with LLaMA2 7B, with only about 40% of computations. Further, our\npreliminary efforts to scale up DeepSeekMoE to 145B parameters consistently\nvalidate its substantial advantages over the GShard architecture, and show its\nperformance comparable with DeepSeek 67B, using only 28.5% (maybe even 18.2%)\nof computations.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2401.02954v1",
    "title": "DeepSeek LLM: Scaling Open-Source Language Models with Longtermism",
    "publish_time": "20240105",
    "authors": [
      "DeepSeek-AI",
      ":",
      "Xiao Bi",
      "Deli Chen",
      "Guanting Chen",
      "Shanhuang Chen",
      "Damai Dai",
      "Chengqi Deng",
      "Honghui Ding",
      "Kai Dong",
      "Qiushi Du",
      "Zhe Fu",
      "Huazuo Gao",
      "Kaige Gao",
      "Wenjun Gao",
      "Ruiqi Ge",
      "Kang Guan",
      "Daya Guo",
      "Jianzhong Guo",
      "Guangbo Hao",
      "Zhewen Hao",
      "Ying He",
      "Wenjie Hu",
      "Panpan Huang",
      "Erhang Li",
      "Guowei Li",
      "Jiashi Li",
      "Yao Li",
      "Y. K. Li",
      "Wenfeng Liang",
      "Fangyun Lin",
      "A. X. Liu",
      "Bo Liu",
      "Wen Liu",
      "Xiaodong Liu",
      "Xin Liu",
      "Yiyuan Liu",
      "Haoyu Lu",
      "Shanghao Lu",
      "Fuli Luo",
      "Shirong Ma",
      "Xiaotao Nie",
      "Tian Pei",
      "Yishi Piao",
      "Junjie Qiu",
      "Hui Qu",
      "Tongzheng Ren",
      "Zehui Ren",
      "Chong Ruan",
      "Zhangli Sha",
      "Zhihong Shao",
      "Junxiao Song",
      "Xuecheng Su",
      "Jingxiang Sun",
      "Yaofeng Sun",
      "Minghui Tang",
      "Bingxuan Wang",
      "Peiyi Wang",
      "Shiyu Wang",
      "Yaohui Wang",
      "Yongji Wang",
      "Tong Wu",
      "Y. Wu",
      "Xin Xie",
      "Zhenda Xie",
      "Ziwei Xie",
      "Yiliang Xiong",
      "Hanwei Xu",
      "R. X. Xu",
      "Yanhong Xu",
      "Dejian Yang",
      "Yuxiang You",
      "Shuiping Yu",
      "Xingkai Yu",
      "B. Zhang",
      "Haowei Zhang",
      "Lecong Zhang",
      "Liyue Zhang",
      "Mingchuan Zhang",
      "Minghua Zhang",
      "Wentao Zhang",
      "Yichao Zhang",
      "Chenggang Zhao",
      "Yao Zhao",
      "Shangyan Zhou",
      "Shunfeng Zhou",
      "Qihao Zhu",
      "Yuheng Zou"
    ],
    "abstract": "The rapid development of open-source large language models (LLMs) has been\ntruly remarkable. However, the scaling law described in previous literature\npresents varying conclusions, which casts a dark cloud over scaling LLMs. We\ndelve into the study of scaling laws and present our distinctive findings that\nfacilitate scaling of large scale models in two commonly used open-source\nconfigurations, 7B and 67B. Guided by the scaling laws, we introduce DeepSeek\nLLM, a project dedicated to advancing open-source language models with a\nlong-term perspective. To support the pre-training phase, we have developed a\ndataset that currently consists of 2 trillion tokens and is continuously\nexpanding. We further conduct supervised fine-tuning (SFT) and Direct\nPreference Optimization (DPO) on DeepSeek LLM Base models, resulting in the\ncreation of DeepSeek Chat models. Our evaluation results demonstrate that\nDeepSeek LLM 67B surpasses LLaMA-2 70B on various benchmarks, particularly in\nthe domains of code, mathematics, and reasoning. Furthermore, open-ended\nevaluations reveal that DeepSeek LLM 67B Chat exhibits superior performance\ncompared to GPT-3.5.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2312.17080v4",
    "title": "MR-GSM8K: A Meta-Reasoning Benchmark for Large Language Model Evaluation",
    "publish_time": "20231228",
    "authors": [
      "Zhongshen Zeng",
      "Pengguang Chen",
      "Shu Liu",
      "Haiyun Jiang",
      "Jiaya Jia"
    ],
    "abstract": "In this work, we introduce a novel evaluation paradigm for Large Language\nModels (LLMs) that compels them to transition from a traditional\nquestion-answering role, akin to a student, to a solution-scoring role, akin to\na teacher. This paradigm, focusing on \"reasoning about reasoning,\" hence termed\nmeta-reasoning, shifts the emphasis from result-oriented assessments, which\noften neglect the reasoning process, to a more comprehensive evaluation that\neffectively distinguishes between the cognitive capabilities of different\nmodels. By applying this paradigm in the GSM8K dataset, we have developed the\nMR-GSM8K benchmark. Our extensive analysis includes several state-of-the-art\nmodels from both open-source and commercial domains, uncovering fundamental\ndeficiencies in their training and evaluation methodologies. Notably, while\nmodels like Deepseek-v2 and Claude3-Sonnet closely competed with GPT-4 in\nGSM8K, their performance disparities expanded dramatically in MR-GSM8K, with\ndifferences widening to over 20 absolute points, underscoring the significant\nchallenge posed by our meta-reasoning approach.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2311.18743v4",
    "title": "AlignBench: Benchmarking Chinese Alignment of Large Language Models",
    "publish_time": "20231130",
    "authors": [
      "Xiao Liu",
      "Xuanyu Lei",
      "Shengyuan Wang",
      "Yue Huang",
      "Zhuoer Feng",
      "Bosi Wen",
      "Jiale Cheng",
      "Pei Ke",
      "Yifan Xu",
      "Weng Lam Tam",
      "Xiaohan Zhang",
      "Lichao Sun",
      "Xiaotao Gu",
      "Hongning Wang",
      "Jing Zhang",
      "Minlie Huang",
      "Yuxiao Dong",
      "Jie Tang"
    ],
    "abstract": "Alignment has become a critical step for instruction-tuned Large Language\nModels (LLMs) to become helpful assistants. However, the effective evaluation\nof alignment for emerging Chinese LLMs is still largely unexplored. To fill in\nthis gap, we introduce AlignBench, a comprehensive multi-dimensional benchmark\nfor evaluating LLMs' alignment in Chinese. We design a human-in-the-loop data\ncuration pipeline, containing eight main categories, 683 real-scenario rooted\nqueries and corresponding human verified references. To ensure the correctness\nof references, each knowledge-intensive query is accompanied with evidences\ncollected from reliable web sources (including URLs and quotations) by our\nannotators. For automatic evaluation, our benchmark employs a rule-calibrated\nmulti-dimensional LLM-as-Judge~\\cite{zheng2023judging} approach with\nChain-of-Thought to generate explanations and final ratings, ensuring high\nreliability and interpretability. All evaluation code, data, and LLM\ngenerations are available at \\url{https://github.com/THUDM/AlignBench}. Since\nits release, AlignBench has been adopted by top (Chinese) LLMs for evaluating\ntheir alignment capabilities in Chinese, including ChatGLM, Qwen, DeepSeek, Yi,\nBaichuan, and Abab.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2310.16818v2",
    "title": "DreamCraft3D: Hierarchical 3D Generation with Bootstrapped Diffusion Prior",
    "publish_time": "20231025",
    "authors": [
      "Jingxiang Sun",
      "Bo Zhang",
      "Ruizhi Shao",
      "Lizhen Wang",
      "Wen Liu",
      "Zhenda Xie",
      "Yebin Liu"
    ],
    "abstract": "We present DreamCraft3D, a hierarchical 3D content generation method that\nproduces high-fidelity and coherent 3D objects. We tackle the problem by\nleveraging a 2D reference image to guide the stages of geometry sculpting and\ntexture boosting. A central focus of this work is to address the consistency\nissue that existing works encounter. To sculpt geometries that render\ncoherently, we perform score distillation sampling via a view-dependent\ndiffusion model. This 3D prior, alongside several training strategies,\nprioritizes the geometry consistency but compromises the texture fidelity. We\nfurther propose Bootstrapped Score Distillation to specifically boost the\ntexture. We train a personalized diffusion model, Dreambooth, on the augmented\nrenderings of the scene, imbuing it with 3D knowledge of the scene being\noptimized. The score distillation from this 3D-aware diffusion prior provides\nview-consistent guidance for the scene. Notably, through an alternating\noptimization of the diffusion prior and 3D scene representation, we achieve\nmutually reinforcing improvements: the optimized 3D scene aids in training the\nscene-specific diffusion model, which offers increasingly view-consistent\nguidance for 3D optimization. The optimization is thus bootstrapped and leads\nto substantial texture boosting. With tailored 3D priors throughout the\nhierarchical generation, DreamCraft3D generates coherent 3D objects with\nphotorealistic renderings, advancing the state-of-the-art in 3D content\ngeneration. Code available at https://github.com/deepseek-ai/DreamCraft3D.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2310.04963v3",
    "title": "LLM4VV: Developing LLM-Driven Testsuite for Compiler Validation",
    "publish_time": "20231008",
    "authors": [
      "Christian Munley",
      "Aaron Jarmusch",
      "Sunita Chandrasekaran"
    ],
    "abstract": "Large language models (LLMs) are a new and powerful tool for a wide span of\napplications involving natural language and demonstrate impressive code\ngeneration abilities. The goal of this work is to automatically generate tests\nand use these tests to validate and verify compiler implementations of a\ndirective-based parallel programming paradigm, OpenACC. To do so, in this\npaper, we explore the capabilities of state-of-the-art LLMs, including\nopen-source LLMs -- Meta Codellama, Phind fine-tuned version of Codellama,\nDeepseek Deepseek Coder and closed-source LLMs -- OpenAI GPT-3.5-Turbo and\nGPT-4-Turbo. We further fine-tuned the open-source LLMs and GPT-3.5-Turbo using\nour own testsuite dataset along with using the OpenACC specification. We also\nexplored these LLMs using various prompt engineering techniques that include\ncode template, template with retrieval-augmented generation (RAG), one-shot\nexample, one-shot with RAG, expressive prompt with code template and RAG. This\npaper highlights our findings from over 5000 tests generated via all the above\nmentioned methods. Our contributions include: (a) exploring the capabilities of\nthe latest and relevant LLMs for code generation, (b) investigating fine-tuning\nand prompt methods, and (c) analyzing the outcome of LLMs generated tests\nincluding manually analysis of representative set of tests. We found the LLM\nDeepseek-Coder-33b-Instruct produced the most passing tests followed by\nGPT-4-Turbo.",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/2309.16120v3",
    "title": "Fixing Large Language Models' Specification Misunderstanding for Better Code Generation",
    "publish_time": "20230928",
    "authors": [
      "Zhao Tian",
      "Junjie Chen",
      "Xiangyu Zhang"
    ],
    "abstract": "Code generation is to automatically generate source code conforming to a\ngiven programming specification, which has received extensive attention\nespecially with the development of large language models (LLMs). Due to the\ninherent difficulty of code generation, the code generated by LLMs may not be\naligned with the specification. Although thought-eliciting prompting techniques\nhave been proposed to enhance the code generation performance of LLMs,\nproducing correct understanding for complicated programming problems remains\nchallenging, resulting in unsatisfactory performance. Also, some feedback-based\nprompting techniques have been proposed to fix incorrect code using error\nmessages produced by test execution. However, when the generated code deviates\nsignificantly from the ground truth, they encounter difficulties in improving\nperformance based on such coarse-grained information. In this work, we propose\na novel prompting technique, called {\\mu}FiX, to improve the code generation\nperformance of LLMs by devising both sophisticated thought-eliciting prompting\nand feedback-based prompting and making the first exploration on their synergy.\nIt first exploits test case analysis to obtain specification understanding and\nenables a self-improvement process to identify and refine the misunderstanding\nin the thought-eliciting prompting phase. {\\mu}FiX further fixes the\nspecification understanding towards the direction reducing the gap between the\nprovided understanding (from the first phase) and the actual understanding\nimplicitly utilized by LLMs for code generation in the feedback-based prompting\nphase. By improving the understanding with {\\mu}FiX, the code generation\nperformance of LLMs can be largely improved. Our evaluation on two advanced\nLLMs (ChatGPT and DeepSeek-Coder) with six widely-used benchmarks by comparing\nwith 15 baselines, demonstrates the effectiveness of {\\mu}FiX ...",
    "score": 1.0
  },
  {
    "link": "http://arxiv.org/abs/1801.03406v2",
    "title": "DeepSeek: Content Based Image Search & Retrieval",
    "publish_time": "20180109",
    "authors": [
      "Tanya Piplani",
      "David Bamman"
    ],
    "abstract": "Most of the internet today is composed of digital media that includes videos\nand images. With pixels becoming the currency in which most transactions happen\non the internet, it is becoming increasingly important to have a way of\nbrowsing through this ocean of information with relative ease. YouTube has 400\nhours of video uploaded every minute and many million images are browsed on\nInstagram, Facebook, etc. Inspired by recent advances in the field of deep\nlearning and success that it has gained on various problems like image\ncaptioning and, machine translation , word2vec , skip thoughts, etc, we present\nDeepSeek a natural language processing based deep learning model that allows\nusers to enter a description of the kind of images that they want to search,\nand in response the system retrieves all the images that semantically and\ncontextually relate to the query. Two approaches are described in the following\nsections.",
    "score": 1.0
  }
]